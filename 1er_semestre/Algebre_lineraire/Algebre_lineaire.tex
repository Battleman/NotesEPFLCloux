\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{textcomp}





%opening
\title{Algèbre linéaire}
\author{Brunner Loïc}

\begin{document}

\maketitle


\section{introduction}
mail:dimitar.jetchev@epfl.ch
\newline
moodle.epfl.ch-> clé: INSC-etudiant
\newline
exercices: lundi 15:15->17h voir le moodle dès le mercredi + corrigé de la semaine précédente
\newline
examen final: 100\%, 20 questions à choix multiple+ 20 vrai ou faux
\newline
livre: algèbre linéaire et Applications (4ème edition) Pearson
\section{systèmes d'équations linéaires}
\subsection{exemples}
forme générale:$a_1x_1+a_2x_2+...+a_nx_n = b$
exemple:
\begin{eqnarray}
 4x_1-5x_2 & = & 2\\
 3x_1+x_2 & = & \sqrt[2]{2}\\
\end{eqnarray}
Savoir dissiner et comprendre un graphe provenant de systèmes d'équations(si on remplace une des variables par un nombre
\paragraph{3 cas}
\begin{enumerate}
 \item il n'y a qu'une seule solution (une seule intersection entre les droites)
 \item les deux droites sont paralleles(pas de solution car pas d'intersection entre les droites)(pas compatibles/pas consistant)
 \item les deux droite sont confondues (infinité de solutions)
\end{enumerate}
Nous voyons donc qu'il y a trois sortes de solutions a un système d'equations a 2 inconnues. Ce qui se voit très bien si on écrit les équation sous forme géométrique
, en prenant $x_1 et x_2$ comme axes 
\subsection{notation matricielle}
\[
\begin{pmatrix}%
1 & -2 & 1 & 0\cr
0 & 2 & -8 & 8\cr
-4 & 5 & 9 & 9 
\end{pmatrix}
\]
 les trois premières colones sont appellées la matrice du système alors que toutes les colones forment la matrice complète

\subsection{résolution des systèmes}

\begin{eqnarray}
 x_1+x_2-3x_3 & = & -1\\
 2x_2-x_3 & = & 1\\
 3x_1-x_2+2x_3 & = & 4\\
\end{eqnarray}
Une élimation des 3 variables peut être envisagée en soustayant 3 fois la troisième equation à la première. Puis en comparant les équations restantes avec les $x_2 et x_3$. en répétant le procesus, nous pouvons trouver une variable et en déduire les autres
\paragraph{}
D'autres solutions éxistent, qui sont plus facilement programmables sur un ordinateur! 

\subsection{équations élémentaires}
\begin{enumerate}
 \item remplacement (je peut remplacer la somme d'une équation avec une autre)
 \item échange (je changer les éqautions entre elles)
 \item multiplication par un scalaire
\end{enumerate}
Se référer a résolution des systèmes pour voir un exemple de ces différentes opérations

\subsection{taille de l'ensemble des solutions}
 Comme nous l'avons vu, il y a trois solutions pour les systèmes d'équations. Comment détermier la taille de l'ensemble solution avecl les opérations élementaires
 \begin{eqnarray}
  x_1-3x_2 & = & 1\\
  2x_1-6x_2 & = & 3\\
 \end{eqnarray}
On peut multiplier la première ligne par -2 et ajouter le résultat à la seconde équation. On continue la résotion de manière classique et on obtient 0=1. ON en déduit qu'il n'y a pas de solutions. Si on obtenait une solution du style 0=0, on verrait alors qu'il y a une 
infinité de solution.

\subsection{Algorithme de Gauss}
Nous allons prendre cette matrice comme matrice étalon:
\[
\begin{pmatrix}%
 1&1&-3&-1\cr
 0&2&-1&1\cr
 3&-1&2&4 
 \end{pmatrix}
\]
 \subsubsection{quelques notions}
\begin{description}
 \item matrice sous forme échelonée:\\{si elle satisfait ces conditions ci dessous:}
 \item coeficient principal:\\{premier élément non nul de gauche a droite sur la première ligne de la matrice}
 \item matrice sous échelonée réduite:\\{il faut qu'elle soit échelonée et que les coefs principaux des lignes non-nulles soit = 1 et que les coeficients principaux soient les seuls éléments non-nuls dans leur colone}
 \item pivot:\\{position d'un emplacement dans la matrice qui correspond à un coef. principal(=1) de la forme échelonnée réduite}
 \end{description}
conditions pour matrice échelonée:
\begin{enumerate}
 \item si les lignes non-nulles sont au dessus des lignes nulles
 \item si le coeficient principale de chaque ligne se trouve a droite du coefiencient principal de la ligne au desssus(ce qui donne à la matrice une forme triangualire)
 \item Tous les coeficients dans une colone en dessus d'un coeficient principal sont nuls(cf. condition 2)
\end{enumerate}
Exemple de matrice échelonée:
\[
\begin{pmatrix}%
 1&2&3\cr
 0&1&2\cr
 0&0&0
\end{pmatrix}
\]

Exemple de matrice échelonée réduite:

\[
\begin{pmatrix}%
 1&0&0&3\cr
 0&1&0&2\cr
 0&0&1&1
\end{pmatrix}
\]

Nous notons que dans cet exemple, la position des 3 premiers 1 est apellés pivot.
\subsubsection{première étape de l'algoritme}
Transformer la matrice sous forme échelonnée.Prenons comme exemple:

\[
\begin{pmatrix}%
 0&3&-6&6&4&-5\cr
 3&-7&8&-5&8&9\cr
 3&-9&12&-9&6&15
 \end{pmatrix}
\]
 On commence par choisir la colone  non-nulle à gauche (ici, la deuxième ligne). On fait l'échange avec la première ligne tel que l'élément non-nul dans la colone 1 soit dans la première ligne.
 
\subsubsection{seconde étape de l'algoritme}
Transformer la matrice sous forme échelonée réduite. Après les transformations ci-dessus, nous  arrivons dans la situation suivante:

\[
\begin{pmatrix}%
 3&-9&12&-9&6&15\cr
 3&-7&8&-5&8&9\cr
 0&3&-6&6&4&-5
 \end{pmatrix}
\]
il faut mnt annuler les éléments qui sont à gauche par soustraciton des lignes entres elles.

\[
\begin{pmatrix}%
 3&-9&12&-9&6&15\cr
 0&2&-4&4&-2&-6\cr
 0&3&-6&6&4&-5
 \end{pmatrix}
\]
\subsubsection{suite algoritme}

Puis on continue a répéter l'étape 1 et 2 jusqu'à ce que la matrice devienne échelonnées. pour obtenir finalement:

\[
\begin{pmatrix}%
 3&-9&12&-9&6&15\cr
 0&2&-4&4&-2&-6\cr
 0&0&0&0&7&4
 \end{pmatrix}
\]

Notre matrice est mnt échelonnée. Ainsi, nous connaissons tous les pivots.
\subsubsection{réduction de la matrice}
 Nous commençons par diviser toutes les lignes pour obtenir des pivots égaux a 1.
 \[
\begin{pmatrix}%
 1&-3&4&-3&2&5\cr
 0&1&-2&2&-1&-3\cr
 0&0&0&0&1&4
 \end{pmatrix}
\]
 
 Ensuite, nous choisissons la colonne de pivot la plus à droite pour commencer les soustractions et obtenir des 0 au de dessus de tous les pivots ce qui nous donne:
 
  \[
\begin{pmatrix}%
 1&-3&4&-3&0&-3\cr
 0&1&-2&2&0&-7\cr
 0&0&0&0&1&4
 \end{pmatrix}
\]
 
   \[
\begin{pmatrix}%
 1&0&-2&3&0&-27\cr
 0&1&-2&2&0&-7\cr
 0&0&0&0&1&4
 \end{pmatrix}
\]
 

 Quel que soit l'ordre dans lequel on effectue les opérations, on obtient la même matrice échelonnée réduite a la fin.
 Nous comprenons qu'il est plus facile de programmer cette méthode de résulution. NOus notons que l'échelonnage de la matrice est plus cher en terme de compexité de programation que sa réduction.
 
 \subsection{Système d'équations linéaires}
 
 La matrice prise comme exemple dans l'algo de Gauss représente un systeme d'équations a 5 inconnues.
 
 \begin{eqnarray}
  3x_2-6x_3+6x_4+4x_5=5\cr
  3x_1-7x_2+8x_3-5x_4+8x_5=9\cr
  3x_1-9x_2+12x_3-9x_4+6x_5=15
 \end{eqnarray}
Sous forme échelonnée réduite, nou savons comme système d'équation linéaires:
\begin{eqnarray}
x_1-2x_3+3x_1=-24\cr
x_2-2x_3+2x_4=-7\cr
x_5=4
\end{eqnarray}
 
 \begin{description}
  \item variables principales:\\{variables qui correspondent au pivot de la matrice échelonnées réduite$(x_1,x_2,x_5)$}
  \item variables secondaires:\\{on peut exprimer les variable principales uniquement avec les variables secondaires$(x_3,x_4)$}
 \end{description}

 \begin{eqnarray}
x_1=2x_3-3x_4-24\cr
x_2=2x_3-2x_4-7\cr
x_5=4
\end{eqnarray}

\subsubsection{exemple}
\begin{eqnarray}
 x_1-2x_2=1\cr
 -2x_1+4x_2=1
\end{eqnarray}
Que l'on transforme en matrice:
\[
\begin{pmatrix}
 1&-2&1\cr
 -2&4&1
\end{pmatrix}
\]
on effectue l'algo de Gauss:

\[
\begin{pmatrix}
 1&-2&1\cr
 0&0&1
\end{pmatrix}
\]
On soustrait la première à la deuxième.

 \[
\begin{pmatrix}
 1&-2&0\cr
 0&0&1
\end{pmatrix}
\]
on soustrait la deuxième à la première.
\paragraph{}
Nous voyons que le système n'a pas de solution.

\begin{description}
 \item important:\\{le système possède une solution et seulement une solution si la fomre échelonnée réduite ne contient pas un pivot deans la dernière colonne}
\end{description}

\section{équation vectorielles}
Chaque vecteur représente un point dans$ R^n$.

\includegraphics{/home/logeek04/Documents/etude/algebre/schema3.jpg}


opérations sur les vecteur:
\begin{itemize}
 \item multiplication par un scalaire
 \item addition entre les vecteurs $ u,v\in R^2\rightarrow u+v\in R^2 $
 \item produit scalaire
\end{itemize}
interprétation géométrique de l'addition: la diagonale du parallelogramme formé par les deux vecteurs.($u-v$ donne l'autre diagonale).

\includegraphics{/home/logeek04/Documents/etude/algebre/schema4.jpg}

\subsection{vercteurs dans $R^n$}

\begin{math}
\begin{pmatrix}
 x_1\cr
 ...\cr
 x_n
\end{pmatrix}
\end{math}

\subsection{combinaisons linéaires}
Soit $v_1,v_2,v_3,...,v_r\in R^n$
\newline
une combinaison linéaire est un vecteur de la forme $c_1 v_1 + c_2 v_2 +... + c_r v_r$
\paragraph{}
\subsubsection{interprétation géométrique}


Si, au contraire, nous prenons $c_1,c_2\in R$ nous n'obtenons plus une série de droites, mais un plan. Ainsi, nous pouvons retrouver n'importe quel vesteur par une combinaison linéaire de vecteurs indépendants.

\subsubsection{équations vectorielles}
 Soit $v_1,v_2,...v_r\in R^n$
 \begin{description}
  \item Vect\{$a_1,...,a_r$\} = $ \{c_1a_1+...+c_ra_r| c_r\in R\}$:\\{la partie de $R^n$ engendrée par $a_1,...,a_r$(somme des combinaisons linéaires)}
 \end{description}
Etant donné $ b\in R^n$, $b\in Vect\{a_1,...a_r\}$? Oui car on peut écrire:
\[b = c_1a_1+c_2a_2+....+c_3a_3\]

\section{les équations matricielles}
Si on exrime une équation linéaire de veceurs sous forme de matrice, on obtient une équation matricielle.
\newline
Soit a la matrice de l'équation vectorille, on a 
\newline
$A$\begin{tabular}{|c|}
    $x_1$\cr
    ...\cr
    $x_r$
   \end{tabular}
= b, $\forall Ax\in b$
\newline
c'est ce qu'on apelle une équation matricielle. Mais qu'elle que soit l'équation que l'on utilise, on obtient le même ensemble solution.
Cette équation en particulier nous permet de déterminer si le vecteur b appartient à la partie de $R^n$ engendrée par les colonnes de la matrice.
Ce qui revient a dire:
\newline
\[b\in Vect\{v_1,...,v_n\}\]
\newline
A=$(v_1,...,v_r)$    Ax=b
\paragraph{}
Pour être capable d'écrire $Ax\in R^n$ il faut que r=m.
\paragraph{exemple}
$v_1,v_2,v_3\in R^3$
\begin{eqnarray}
 3v_1-5v_2+7v_3=b
\end{eqnarray}
Il faut trouver l'équation matricielle en écrivant différemment l'équation vectorielle.

\begin{math}
A(3x3):
\begin{pmatrix}
 v_1&v_2&v_3
\end{pmatrix}
x:
\begin{pmatrix}
 3\cr
 -5\cr
 7\cr
\end{pmatrix}
\end{math}
Nous vérifions ainsi l'équation Ax=b. Pour calculer Ax ou A est nxr et x est rx1, l arpgle est:
\newline
Le résultat est un veceur nx1 ou la ième composantes est la somme des produits des éléments de la ligne $n^o$ i et des différents éléments du vercteur x.
\subsection{propriétés du produit matrice/vecteur}
\paragraph{théorème}
Soit A une matrice nxr, $u,v\in R^r$, $c\in R$.
\begin{enumerate}
 \item A(u+v)=Au($\in R^n$)+Av8$\in R^n$)
 \item A(cu)=cAu
\end{enumerate}

\paragraph{existance des solutions}
Comment savoir s'il existe des équations matricielles.
\newline
$Ax=b$
\newline
les conditions suivantes doivent être respectées:
\begin{enumerate}
 \item il existe $ x\in R^r$ de l'équation matricielle.
 \item $b\in Vect\{a_1,...,a_r\}$ ou A=\{$a_1,...a_r$\}
 \item il existe une soltuion de l'équation vectorielle $x_1a1+x_2a_2+...+x_ra_r=b$
\end{enumerate}
exemple:
$A=
\begin{pmatrix}
 1&2&-1\cr
 -2&2&0\cr
 4&-1&3\cr
\end{pmatrix}
b=
\begin{pmatrix}
 b_1\cr
 b_2\cr
 b_3
\end{pmatrix}
$
\newline
Question: l'équation Ax=b est-elle compatible pour tous les $b_1,b_2,b_3$ possibles?
\newline
On écrit la matricce complète pour faire l'élimination de Gauss:
\[
\begin{pmatrix}
  1&2&-1&b_1\cr
 -2&2&0&b_2\cr
 4&-1&3&b_3\cr
\end{pmatrix}
\]
 \newline
 Ce qui nous donne à la fin:
 \[
\begin{pmatrix}
  1&-2&-1&b_1\cr
 0&1&1&-b_2/2-b_1\cr
 0&0&0&3b_1+7/2b_2+b_3\cr
\end{pmatrix}
\]
\newline
On a une solution si est seulement si $3b_1+\frac{7}{2}b_2+b_3=0$, donc l'équation n'est pas compatible pour tous les $b_1,b_2,b_3$ possibles.
\paragraph{théorème}
Soit A une matrice nxr.
\newline
Les propriétés suivantes sont équivalentes:
\begin{itemize}
 \item Pour tout $b\in R^n$, l'équation Ax=b possède une solution.
 \item Tout vecteur b de $R^n$ est un ecombinaison linéaire des colonnes de A
 \item Les colonnes de A engendrent $R^n$,$Vect\{a_1,...,a_r\}$
\end{itemize}

$n
\begin{pmatrix}
 a_1&...&a_r\cr
\end{pmatrix}
\begin{pmatrix}
 x_1\cr
 ...\cr
 x_r\cr
\end{pmatrix}
=
\begin{pmatrix}
 b_1\cr
 ...\cr
 a_r\cr
\end{pmatrix}
$

Il faut prouver l'équivalence des\newline trois propositions:
\newline
Il existe un b tel que Ax=b n'est pas compatible.
\newline
$
\begin{pmatrix}
 A&b\cr
\end{pmatrix}
\sim(Gauss)
\begin{pmatrix}
 v&d\cr
\end{pmatrix}
$
\newline
La dernière matrice étant la forme échelonée réduite. Il n'existe pas solution si et seulement si la dernière colonne est une colonne de pivot.

\section{les systèmes linéaires Homogène/non-homogènes}
\subsection{équations homogènes}
\begin{eqnarray}
x_1-2x_2+3x_3=0\cr
-2x_1-3x_2-4x_3=0\cr
2x_1-4x_2+9x_3=0\cr
\end{eqnarray}
Ax=b
\newline
$b=
\begin{pmatrix}
 0\cr
 0\cr
 0\cr
\end{pmatrix}
$
\begin{enumerate}
 \item pout b= 0 il y a toujours une solution
 \item l'origine appartient à Vect\{$a_1,...,a_r$\}
 \item (0,0,0) est une combinaison linéaire des colonnes
\end{enumerate}
en résolvant on trouve:
\newline
$
\begin{pmatrix}
 1&0&0&0\cr
 0&1&0&0\cr
 0&0&1&0\cr
\end{pmatrix}
$
On voit donc ainsi l'origine de la solution triviale.
\subsection{équations non-homogènes}

\begin{eqnarray}
x_1-3x_2-8x_3=5\cr
x_2+2x_3=-4\cr
\end{eqnarray}
la matrice échelonnée réduite nous donne:
\[
\begin{pmatrix}
 1&0&-2&-7\cr
 0&1&2&-4\cr
\end{pmatrix}
\]
\newline
\begin{eqnarray}
 x_2=-4-2x_3\cr
 x_1=-7+2x_3\cr
 x_3=1
\end{eqnarray}
Ainsi peut trouver une solution particulière(5,6,1). Nous allons utiliser cette solution particulère pour trouver la solution générale du système homogène.
\newline
Il faut commencer par exprimer la solution générale (en remplaçant le vecteur b par le vecteur nul):
\newline
$
x_3
\begin{pmatrix}
2\cr
-2\cr
1\cr
\end{pmatrix}
$



alors la sulution générale:
\[
\begin{pmatrix}
-5\cr
-6\cr
1\cr
\end{pmatrix}
+
\begin{pmatrix}
2\cr
-2\cr
1\cr
\end{pmatrix}
x_3=
\begin{pmatrix}
-5 +2x_3\cr
-6-2x_3\cr
1+x_3\cr
\end{pmatrix}
\]
\subsection{indépendence linéaire}
\[
\begin{pmatrix}
1&3&2\cr
-1&1&0\cr
1&1&1\cr
\end{pmatrix}
\begin{pmatrix}
x_1\cr
x_2\cr
x_3\cr
\end{pmatrix}
=0
\]
\newline
On a $x_1=x_2=x_3=0$
comme une solution:
\[
x_1
\begin{pmatrix}
1\cr
-1\cr
1\cr
\end{pmatrix}
+x_2
\begin{pmatrix}
3\cr
1\cr
1\cr
\end{pmatrix}
+x_3
\begin{pmatrix}
2\cr
0\cr
1\cr
\end{pmatrix}
=0
\]
\newline

la solution triviale n'est pas la seule solution!!
\begin{description}
\item système lié /linéairement dépendant:\\{s'il existe $x_1,x_2,...,x_r\in R,t.q.(x_1,...,x_r)\neq(0,...,0) et x_1v_1+...+x_rv_r=0$}
\item systeme libre/linéairement indépendant:\\{si l'équation vectrorielle $x_1v_1+x_2v_2+...+x_rv_r=0$ n'admet que $x_1=...=x_r=0$ comme solution}
\end{description}
\subsubsection{indépendance linéaire pour les colonnes d'une matrice}
Si on peut écrire une colonne comme étant x* une autre, les colonnes sont linéairement dépandantes.
\newline
Les colonnes sont linéairement indépendantes si et seulement si l'équation Ax=0 admet la solution triviale comme solution unique.
\subsubsection{Famille d'au moins deux vecteurs}
\paragraph{théorème}
Soit $F=\{v_1,...,v_r\}$ est linéairement dépendante sssi au moins l'un des vecteurs de F est une combinaison linéaire des autres vecteurs.
\newline
Preuve:
\newline
Si F est liée, il existe $(c_1,...,c_r)\neq (0,...,0)/c_1v_1+...+c_rv_r=0$ $v_1=\frac{c_2}{c_1}v_2+...+\frac{c_r}{c_1}v_r$
...
\paragraph{exemple}
Soit u=
\[
\begin{pmatrix}
1\cr
1\cr
2\cr
\end{pmatrix}
\]
,v=
\[
\begin{pmatrix}
2\cr
0\cr
1\cr
\end{pmatrix}
\]
$\in R^3$
\begin{enumerate}
 \item déterminer la partie de $R^3$ Vect\{u,v\}(plan)
 \item Si $w\in R^3$ alors w$\in Vect\{u,v\}$ sssi u,v,w sont linéairement dépendants
\end{enumerate}
w=cu +dv,$c,d\in R$
\newline
\{u,v,w\} sont linéairement dépendants
\newline
$\exists(c_1,c_2,c_3)\neq(0,0,0)/c_u+c_2v+c_3w=0$
\newline
$c_3=0\Rightarrow c_u+c_2v=0$ contradiction car u et v ne sont pas colinéaires et la partie de $R^3$ qu'ils engendrent est un plan. Ce qui implique qu e$c_3$ n'est pas égal à 0.
\newline
Donc:
$w=-\frac{c_1}{c_3}u-\frac{c_2}{c_3}v$ et les trois vecteurs sont linéairement dépendants.
\paragraph{théorème}
UNe famille de vecteurs ayant strictement plus de vecteurs que la dimension de l'espace est liée
\newline
$v_1\in R^n$ $v_r\in R^n/r>n$
\newline
Preuve:
\[
\begin{pmatrix}
v_1|v_2|...|v_r|0\cr
\end{pmatrix}
\]
\newline
Il existe au moins une inconnue non-principale. Ce qui signifique qu'il existe une infinité de solutions. Ce qui implique qu'il existe une solution non nulle ce qui nous montre que la famille est liée/dépendante.
\subsection{les transformations lilnéaires}
Ax=b
\[
\begin{pmatrix}
2&1&0\cr
1&1&1\cr
\end{pmatrix}
\begin{pmatrix}
1\cr
0\cr
o\cr
\end{pmatrix}
=
\begin{pmatrix}
2\cr
1\cr
\end{pmatrix}
\]
\newline
la matrice A agit sur$\begin{pmatrix}
1\cr
0\cr
0\cr
\end{pmatrix}
\rightarrow
\begin{pmatrix}
2\cr
1\cr
\end{pmatrix}$
\newline
$T:R^3\rightarrow R^2$
\newline
on effectue ici une transformation associée à A. C'est A qui agit sur x pour obtenir b. on a donc une transformatin de $R^m\rightarrow R^n$
\newline
\begin{itemize}
 \item Déterminer T(u) pour $u\in R^n$
 \item Déterminer x/ T(x)=b
 \item Existe-t-il plusieurs vecteurs x dont l'image de T soit égale à b
 \item $v\in R^m$, est-il dans l'image de T
\end{itemize}

\paragraph{exemple}
\[
\begin{pmatrix}
1&2\cr
-1&0\cr
2&1\cr
\end{pmatrix}
\begin{pmatrix}
1\cr
1\cr
\end{pmatrix}
=\begin{pmatrix}
3\cr
-1\cr
3\cr
\end{pmatrix}
\]
Après, pour trouver x en fontion de b et de A, il suffit de remplacer les chiffres par $x_1,x_2$. et on fait la matrice échelonnée avec A et b pour les déterminer.
 Pour répondre à la troisème question, il suffit de regarder le résultat de la matrice échelonnée. 
\newline
Il faut voir si le pivot de la matrice échelonneé réduite faite avec la matrice A et v (on remplace le b par v, ce qui signifie que l'on cherche à savoir si v est une solution) est dans la dernière colonne. Si c'est le cas, le system n'est pas compatible et le vecteur v n'est pas dans l'image de T. 
\newline
\paragraph{exemples}
On va essayer d'écrire la matrice qui correspond à la projection de manière géométrique:
\newline
Porjection T:$R^3\rightarrow R^2$
\includegraphics{/home/logeek04/Documents/etude/algebre/schema6.jpg}
\newline
On va essayer d'écrire la matrice qui correspond à cette application:
\newline
\[
A=\begin{pmatrix}
1&0&0\cr
0&1&0\cr
\end{pmatrix}
\]
\newline
On peut aussi regarder la projection sur une applcation dans $R^3$. A ce moment là, la matrice nous donne:
\newline
\[
A=\begin{pmatrix}
1&0&0\cr
0&1&0\cr
0&0&0\cr
\end{pmatrix}
\]
\newline
\paragraph{tranvection}
\includegraphics{/home/logeek04/Documents/etude/algebre/schem7.jpg}
On garde la distance du dessus, mais on lui fait faire une translation.
\newline
Dans cet exemple, notre matrice A sera:
\[
\begin{pmatrix}
1&2\cr
0&1\cr
\end{pmatrix}
\]
\newline
Avec cette transvection super cool, tout l'espace se décale et on peut le voir avec d'autres ploint de l'espace. Nous le réutiliserons par la suite.
Elle transforme les vecteurs de cette manière:
\[
\begin{pmatrix}
x+2\cr
1\cr
\end{pmatrix}
\]
\paragraph{rotation}
on peut aussi exprimer la rotation d'un vecteur avec les matrices:
\includegraphics{/home/logeek04/Documents/etude/algebre/schema8.jpg}
\newline
La matrice A sera:
\[
\begin{pmatrix}
0&-1\cr
1&0\cr
\end{pmatrix}
\]
\newline
Ce qui nous permettra de faire une rotation de $90^0$. c'est pire chouette hein? Tu veux savoir comment on exprime les vecteurs?non? je te le dis quand même:
\[
A=\begin{pmatrix}
-x_2\cr
x_1\cr
\end{pmatrix}
\]
\newline
Pour un angle qui n'est pas $90^0$ on va écrire les coordonnées différemment: $(cos\theta, sin \theta)$. On une matrice qui nous transforme tout les veteurs et nous permet de les écrire des vecteurs sous cette forme. 
\[
A=\begin{pmatrix}
cos\theta&sin\theta\cr
sin\theta&cos\theta\cr
\end{pmatrix}
\]
\newline
Dans cette matrice il suffit de remplacer $\theta$ par notre angle en radian.
\paragraph{Homothéthie}
C'est une trsansformation de $R^2$ dans $R^2$. Cette opération consiste juste à faire une dilatation ou une contraction du vecteur...:
\includegraphics{/home/logeek04/Documents/etude/algebre/schema9.jpg}
\newline
Si $r<1$ on a ce que l'on apelle une contraction et si $r>1$ on a une dilatation. On peut voir que la transformation du plusieurs vecteur par l'application homotétie, on obtient des formes qui restent similaires.
\newline
Nous donnons en passant comme ça sans raison les condition pour qu'une applicatino soit linéaire:
\begin{enumerate}
 \item T(u+v) ?T(u)+T(v) $\forall u,v\in R^n$
 \item T(cu)=cT(u) $\forall c\in R,u\in R^n$
\end{enumerate}

Reprenons notre exemple...ou pas...
\paragraph{reflexion}
Une reflexion c'est simplement une symétrie d'axe.
\includegraphics{/home/logeek04/Documents/etude/algebre/schema10.jpg}
\newline
ici, c'est facile la matrice A sera:
\[
\begin{pmatrix}
1&0\cr
0&-1\cr
\end{pmatrix}
\]
\newline
Mais nous pouvons faire également unee réflexion par rapport à l'autre axee ce qui nous donne pour matrice 
\[
\begin{pmatrix}
-1&0\cr
0&1\cr
\end{pmatrix}
\]
\newline
Mais on peut aussi avoir des double reflexion :
\[
\begin{pmatrix}
1&0\cr
0&1\cr
\end{pmatrix}
\]
\paragraph{dilatation}
Une dilatation nous fait faire:
\includegraphics{/home/logeek04/Documents/etude/algebre/schema11.jpg}
Par contre à toi de trouver la matrice A, elle ne nous a pas été donnée...
\subsection{injectivité et surjectivité}
\begin{description}
 \item application injective:\\{si tou vecteur $v\in R^m$ est l'image d'au plus un vecteur de $R^n$}
 \item application surjective:\\{si tous les vecteurs $v\in R^m$ sont l'image d'au moins un vecteur de $R^n$}
 \item application bijective:\\{si tout vecteur $v\in R^m$ est l'image d'exactement un vecteur de $R^n$}
\end{description}
\paragraph{théorème}
Soit:$T:R^n\rightarrow R^m$ une application linéaire associée à A. Alors T est surjective sssi les colonnes de A engendrent $R^n$. Et T est injectif sssi les colonnes de A sont linéairement indépendantes
\newline
preuve:
\newline
1
\newline
$\forall b\in R^m, \exists x/T(x)=b$
\newline
$\Rightarrow Ax=b$
\newline
A=
\[
\begin{pmatrix}
a_1&1_2&...&a_n\cr
\end{pmatrix}
\]
\newline
Alors b= $x_1a_1+...+x_na_n$
\newline
2
\newline
$\Rightarrow$T est injectif ce qui implique de Ax=0 et x=0.alors $\exists (x_1,...,x_n)\neq(0,...,0)/x_1a_1+....+x_na_n=0.$
\newline
$\Longleftarrow$Si les colonnes sont indépendantes c aveut dire: $x_1a_1+...+x_na_n=0$ tel que cette équation a la solution triviale comme la seule solution Ax=0.
\section{matrices}
Attention à la notation, il faut s'y habituer.
\subsection{opérations matricielles}
A(mxn)(m lignes et n colonnes)=
\[
\begin{pmatrix}
a_{11}&...&a_{1n}\cr
a_{21}&...&a_{2n}\cr
...&...&...\cr
a_{m1}&...&a_{mn}\cr
\end{pmatrix}
\]
\newline
les éléments de la diagonales sont toujours noté $a_{ii}$
\subsubsection{addition}
A+B en prends la sommes des les éléments de A et de B: $A+B=(a_{ij}+b{ij})_{i=1,j=1}$
\subsubsection{multiplication}
On parle de multiplication avec un scalaire. On multiplie tous les éléments de la matrice, c'est comme faire la mutiplicaton d'un vecteur par un scalaire si  tu vois ce que je veux dire...;)
\subsubsection{propriétés}
\begin{itemize}
 \item A+B=B+A(commutativité)
 \item (A+B)+C=A+(B+C)(associativité)
 \item A+0=A(élément neutre)
 \item r(A+B)=(rA+rB)(distributivité)
 \item (r+s)A=rA+sA
 \item r(sA)=(rs)A
\end{itemize}
\subsubsection{multiplication matricielle}
$A_{(m,n)},B_{(n,r)}$
A prendre comme une application linéaire.
\newline
Si on prends A(Bx), $x\in R^r=A(x_1b_1+x_2b_2+...+x_rb_r)$, ou $B=[b_1,...,b_r]$
\newline
$=x_1Ab_1|x_2Ab_2|x_3Ab_3|...|x_rAb_r$
\paragraph{}
\[
\begin{pmatrix}
a_{11}&...&a_{1n}\cr
...&...&...\cr
a_{m1}&...&a_{mn}\cr
\end{pmatrix}
\]
\[
\begin{pmatrix}
b_{11}&...&b_{1n}\cr
...&...&...\cr
b_{m1}&...&b_{mn}\cr
\end{pmatrix}
\]
\newline
On multiplie les élémentde de la ième ligne avec ceux de la ième colonne. $(AB)_{ij}=a_{i1}b_{1j}+a_{i2}b{2j}+...+a_{in}b_{nj}$
\newline
Attention: AB$\neq$BA
\paragraph{théorème}
SOit A(mxn) et B(nxr)
\newline
soit $I_m$ la matrice identité (avec que des 1 dans la diagonale)
\begin{itemize}
 \item A(BC)=(AB)C/C est rxk (pour le voir il faut penser en terme de composition d'application linéaires T.T, ou alors tu use de la formule du haut)
 \item A(B+C)=AB+AC/C est nxr(distributivité)
 \item (B+C)A=BA+CA/B,C mxn,A est nxr
 \item $r\in R r(AB)=(rA)B=A(rB)$
 \item A mxn,$I_m,I_n/I_nA=A=I_mA$
 
 \end{itemize}
 
 \subsection{puissance d'une matrice}
 Soit A nxm $A^0=I_n$ et $a^n=A^{n-1}\cdot A$
 \subsection{transposée d'une matrice}
 $A=(a_{ij})_{m,i=1/n,j=1}$ $A^T$ est la matrice mxn dont la ième ligne est la ième colonne de A. 
\newline
A=
 \[
\begin{pmatrix}
1&1&1\cr
2&3&1\cr
\end{pmatrix}
\]
\[A^T
\begin{pmatrix}
1&2\cr
1&3\cr
i&1\cr
\end{pmatrix}
\]
\begin{itemize}
 \item $(A^T)^T=A$
 \item $(A+B)^T=A^T+B^T$
 \item $(rA)^T=rA^T$
 \item A mxn  B nx: $(AB)^T=B^TA^T$(attention au changement de sens)
\end{itemize}
\subsection{inverse d'une matrice}
On dit qu'une matrice A de type nxn est inversible s'il existe une matrice C du type nxn tel que CA=$I_n$=AC
\paragraph{théorème}
Soit A une matrice 
2x2 A=
\[
\begin{pmatrix}
a&b\cr
c&d\cr
\end{pmatrix}
\]
\newline
Ou ad-bc$\neq$0
\newline
Alors A ests inversible et $A^{-1}=\frac{1}{ad-bc}
\begin{pmatrix}
a&-b\cr
-c&a\cr
\end{pmatrix}
$
\newline
Pour vérifier il suffit de multiplier A et C
\newline
si le déterminant de A n'est pas égal a 0 A est inversible
\newline
Si det(A)=0 alors on dit que A est singulière.
\paragraph{algo pour l'inverse d'une matrice}
Soit A une matrice nxn
\newline
Soit $b\in R^n$
\newline
Ax=b on supppose que A est inversible $A^{-1}(Ax)=A^{-1}b$ alors:$x=A^{-1}b$

\subsubsection{méthode pour une matrice nxn}
propriétés:
\begin{itemize}
 \item si A est inversible: $A^{-1}alors:(A^{-1})^{-1}$
 \item AB(avec A et B inversible):$(AB)^{-1}=A^{-1}B^{-1}$
 \item $(A^T)^{-1}=(A^{-1})^T$
\end{itemize}
\paragraph{algorithme d'inversion}
\begin{enumerate}
 \item Echange des lignes
 \item Multiplication par un scalaire
 \item Remplacement
\end{enumerate}
Peut-on écrire ces opération sous forme matricielle/A'=EA(avec A' la matrice optenue après l'opération choisie(échange, Multiplication, remplacement)
\newline
Matrice de trasformation pour l'échange des deux premières ligne d'une matrice 3x3(si tu voit pas ce qu'elle fait prends une matric 3x3 quelconque et fait la multiplicaiton avec cette matrice):
\newline
\[
\begin{pmatrix}
0&1&0\cr
1&0&0\cr
0&0&1\cr
\end{pmatrix}
\]
\newline
Pour la multiplication par un scalaire r pour une matrice 3x3(première ligne):
\[
\begin{pmatrix}
r&0&0\cr
0&1&0\cr
0&0&1\cr
\end{pmatrix}
\]
\newline
pour le remplacement de la dernière ligne avec r fois la première:
\[
\begin{pmatrix}
1&0&0\cr
0&1&0\cr
r&0&1\cr
\end{pmatrix}
\]
\paragraph{}
nous notons que le matrices élémentaires(celles du dessus dans cette section) sont toutes inversibles:
\[
\begin{pmatrix}
0&1&0\cr
1&0&0\cr
0&0&1\cr
\end{pmatrix}
\]
\newline
\[
\begin{pmatrix}
r^{-1}&0&0\cr
0&1&0\cr
0&0&1\cr
\end{pmatrix}
\]
\newline
\[
\begin{pmatrix}
1&0&0\cr
0&1&0\cr
-r&0&1\cr
\end{pmatrix}
\]
\paragraph{inversion}
A est inversible sssi la forme réduite échelonnée de A est la matrice identité. La méthode de Gausse nous permet donc de trouver la matrice identité.
\newline
Si A est inversible $I_n=E_r...E_2E_1A$. On peut exprimer l'identité comme un produit des matrices élémentaires. C'est une facon super cool d'écire la méthode de Gauss.
\newline
On peut donc dire que:
\newline
$\Rightarrow A^{-1}=E_rE_{r-1}...E_1I_n$ A noter que si l'on obtient pas l'identité à gauche, la matrice n'est pas inversible.
\paragraph{ce qui implique}
La méthode de Gauss sur $[A|I_n]$ nous permettra de calculer $a^{-1}$
\paragraph{exemple}
\[
\begin{pmatrix}
0&1&1&1&0&0\cr
1&0&-1&0&1&0\cr
2&1&1&0&0&1\cr
\end{pmatrix}
\]
\newline
Sous forme échelonnée nous donne:
\newline
\[
\begin{pmatrix}
1&0&0&-\frac{1}{2}&0&\frac{1}{2}\cr
0&1&0&\frac{3}{2}&1&-\frac{1}{2}\cr
0&0&1&-\frac{1}{2}&-1&\frac{1}{2}\cr
\end{pmatrix}
\]
\newline
ce qui nous donne la matrice $A^{-1}$:
\newline
\[
\begin{pmatrix}
-\frac{1}{2}&0&\frac{1}{2}\cr
\frac{3}{2}&1&-\frac{1}{2}\cr
\frac{1}{2}&-1&\frac{1}{2}\cr
\end{pmatrix}
\]
\subsubsection{caractérisation des Matrices Inversibles}
Les propriétés suivantes sont équivalentes(elles sont soit toutes vraies, soit toutees fausses):
\begin{enumerate}
 \item A inversible
 \item A est équivalente selon les ligne de $I_n$.
 \item A admet n positions de pivots
 \item Ax=0 n'admet que la solution triviale
 \item Les colonnes de A sont linéairement indépendantes
 \item L'application linéaire $x\rightarrow Ax$ est injective
 \item $\forall b\in R^n,Ax=b$ admet au moins une solution
 \item les colonnes de A engendrent$R^n$
 \item L'applicaiton $x\rightarrow Ax$ est surjective
 \item Il existe C(nxn)/CA=$I_n$
 \item Il existe D(nxn)/AD=$I_n$
 \item $A^T$ est inversible
\end{enumerate}
\subsubsection{les application linéaires inversibles}
\begin{description}
 \item application de $R^n$ dans $R^n$ est inversible:\\{il existe une applicaiton linéaire de $R^n$ dans $R^n$/$S(T(x))=x\forall x\in R$ et $T(S(x))=x\forall x\in R^n$}
\end{description}
\paragraph{théorème}
T est inversible sssi la matrice A de T est inversible. Si T est inversible, alors S:$R^n\mapsto R^n$ est $S(x)=A^{-1}x$
\newline
La preuve ne sera pas complète mais on peut le vérifier e nsupposant que la matrice A est inversible:
\newline
$\exists A^{-1}\Rightarrow S(s):=A^{-1}x$
\newline
T(s(x))=$A(A^{-1}x)=(AA^{-1})x=x$
\newline
S(T(x))=$Ax(A^{-1})=(AA^{-1})x=x$
\newline
ON vient de prouver l'existance de S.
\paragraph{}
Soit A = T et B la matrice associée à la matrice S
\newline
$ABx=x=BAx\forall x\in R^n$
tu as manque un bout du cours mais tu as le livre pour rechercher les infos

\section{factorisation LU}
début à voir dans le livre.
\newline
U=L'A
\newline
A=LU
\newline
sachant que U est la matrice écholonnée de A et L une matrice contenant toutes les opérations de remplacement. L étant l'inverse da la matrice des produit 
que l'on a fait pour obtenir les opérations de la matrice échelonnée.
\newline
Comment calculer L sachant que U=$(E_rE_{r-1}...E_1)A\Rightarrow  A=(E_rE_{r-1}...E_1)^{-1}U=e_r^{-1}E_{r-1}^{-1}...E_1U$
\paragraph{exemple}
\[
\begin{pmatrix}
1&0&0\cr
2&1&0\cr
0&0&1\cr
\end{pmatrix}
\]
\newline
est l'opération qui permet de multiplier par 2 la première colonne et de l'ajouter à la deuxieme ligne. Après, quand on a toutes les $E_k$
il nous suffit de les inverser unes par unes(en gros inverser le signe du 2 dans la matrice de l'exemple) 
\[
\begin{pmatrix}
1&0&0\cr
-2&1&0\cr
0&0&1\cr
\end{pmatrix}
\]
Après l'inversion on multiplie tous les $E_k^{-1}$ entre eux(multiplication matricielle termes à termes) On trouve L.
\newline
il nous suffit d'inverser L pour trouver L'.
\paragraph{astuce}
Pour trouver la matrice L sans perdre trop de temps. En effet, pour trouver L, il suffit de prendre la colone de pivot dans la forme échelonnées et de la 
diviser pour trouver 1 dans le pivot puis tu rajoute ce qu'il y a sous le pivot dans la matrice L.
\paragraph{j'imagine ton mindfuck quand tu lis cette phrase}
A(nxn)dense($\neq0)$ trouve moi $A^{-1}$
\newline
$2n^3$ pour $[A|I]\sim [I|A^{-1}]$ et ca c'est cher.
\newline
A=LU avec la technique du dessus on fait:$\frac{2}{3}n^3$ ce qui nous permet d'éviter un tiers des opérations.
\paragraph{remarque}
Si A est creuse, nous pouvons beaucoup gagner en terme d'efficacité des algorithmes. Si nous calculons $A^{-1}$ nous nous rendons 
compte que cette dernière n'est pas creuse. Dans le cas de LU(avec U creuse), décomposition casr plus rapide de caculer LU.
\subsubsection{exemple}
Soit A une matrice nxn, triangulaire inférieure dont les éléments diagonaux sont non nuls.
\paragraph{à démontrer}
A est une matrice inversible et l'inverse de A est triangulaire inférieure.
\paragraph{exemple}
\[
A=
\begin{pmatrix}
1&0&0\cr
0&1&9\cr
1&0&1\cr
\end{pmatrix}
A^{-1}=
\begin{pmatrix}
1&0&0\cr
0&1&0\cr
-1&0&1\cr
\end{pmatrix}
\]
\paragraph{preuve}
\[
A=
\begin{pmatrix}
d_1&0&0\cr
*&d_2&0\cr
*&*&d_3\cr
\end{pmatrix}
\]
$d_,..d_n\neq 0$
\newline
A est inversible $\Leftrightarrow$ A possède n pivots.
On utilise que des opérations de remplacement et de multiplicaiton scalaire.
\newline
$[A|I]\equiv...\equiv[I|A^{-1}]$
\newline
ces opéraitons appliquées à la matrice $I_n$ alors $A{-1}$ est triangulare inférieure.
\paragraph{}
$[A|I]
\begin{pmatrix}
1&0&0&|&1&0&0\cr
1&1&0&|&0&1&0\cr
1&1&1&|&0&0&1\cr
\end{pmatrix}
\equiv...\equiv
\begin{pmatrix}
1&0&0&|&1&0&0\cr
0&1&0&|&-1&1&0\cr
0&0&1&|&0&-1&1\cr
\end{pmatrix}
$
\subsubsection{factorisation QR(autre exemple)}
Supposons que A=QR avec:
\begin{itemize}
 \item R est inversible et triangulaire supérieure
 \item Q satisfait $Q^{T}Q=I$
\end{itemize}
exemple de Q:
\[
\begin{pmatrix}
cos\theta&sin\theta\cr
-sin\theta&cos\theta\cr
\end{pmatrix}
\begin{pmatrix}
cos\theta&-sin\theta\cr
sin\theta&cos\theta\cr
\end{pmatrix}
=
\begin{pmatrix}
1&0\cr
0&1\cr
\end{pmatrix}
\]
\newline
Supposons qu'on a Ax=b
\begin{enumerate}
 \item la matrice A est-elle inversible?
 \item comment peut-on trouver les solutions?
\end{enumerate}
Q(Rx)=b$\Leftrightarrow Rx=q^{-1}b=Q^{T}b$
\newline
dans ce cas, l'inversion de R n'est pas obligatoire. En effet, il nous suffit de poser et de calculer $y=Q^Tb$.
\newline
Alors on a Rx=y
\[
\begin{pmatrix}
*&*&*\cr
0&*&*\cr
0&0&*\cr
\end{pmatrix}
\begin{pmatrix}
x_1&...&x_n\cr
\end{pmatrix}
=y=
\begin{pmatrix}
y_1&..&y_n\cr
\end{pmatrix}
\]
car R est inversible et Q est inversible.
\paragraph{}
A=QR est inversible $A^{-1}=(QR)^{-1}=R^{-1}Q^{-1}$
\newline
Ce qui implique qu'il existe seulement une seule solution pour Ax=b.
\paragraph{exemple numérique}
\[
\begin{pmatrix}
cos\theta&cos\theta-sin\theta\cr
sin\theta&cos\theta+sin\theta\cr
\end{pmatrix}
Ax=\begin{pmatrix}
1\cr
1\cr
\end{pmatrix}
\]
On cherche x en fonction de$\theta$
\[A=
\begin{pmatrix}
cos\theta&-sin\theta\cr
sin\theta&cos\theta\cr
\end{pmatrix}
\begin{pmatrix}
1&1\cr
0&1\cr
\end{pmatrix}
(QR)
\]
\subsubsection{décomposition des valeurs singulières}
$A=UDV^{T}$, A matrice(nxn)
\begin{itemize}
 \item U,V sont orthogonales,$U^TU=I=V^TV$
 \item D est diagonale dont les éléments diagonaux sont $>0$
\end{itemize}
\paragraph{problème}
Montrer que A est inversible et trouver $A^{-1}$ en utilisant la factorisastion inverse.
$A^{-1}$ les trois matrices D,U,V sont inversibles (D étant diagonale, elle est aussi inversible).
\newline
Ainsi;
$A^{-1}=(UDV^T)=VD^{-1}U^T$
\subsection{sous-espace vecrotiels de $R^n$ }
\begin{description}
 \item sous-espace vectoriel de $R^n$:\\{sous-ensemble $V\subset R^n$ satisfaisant les propriétées suivantes} 
\end{description}
\begin{enumerate}
 \item $O\in V$
 \item $\forall u,v\in V\Rightarrow u+v\in V$
 \item $\forall u\in V et c\subset R\Rightarrow cu\in V$
\end{enumerate}
\subsubsection{image/noyau d'une matrice}
soit A une matrice(nxn) et associée à l'application:
\newline
$T:R^n\rightarrow R^m$
\begin{description}
 \item noyau d'une matrice:\\{sous-ensemble Ker(A)=\{$x\in R:Ax=0$\}}
 \item image d'une matrice:\\{sous-ensemble Im(T)=\{$x\in R^m:\exists \in R^n/T(x)=b$\}}
\end{description}
$Si Ker(A)\subset R^n$
\begin{itemize}
 \item $Ker(a)\in 0$
 \item $Au=0=Av$ car $u+v\in Ker(A)$
 \item $Au=0$ car $A(cu)= c(Au)=0$
\end{itemize}
$T:R^n\mapsto R^m$
\newline
Ker(T) = Ker(A) avec A est la matrice associée à T. 
\newline
Im(A)=$Vect\{a_1,...,a_m\}$ ou $A=[a_1...a_m]$
\subsubsection{base}
Soit $V\subset R^n$ un sous-espace vectoriel
\begin{description}
 \item base de V:\\{une famille $\{v_1,..v_r\}\subset V/V=Vect\{v_1,..,v_r\}$ et la famille est linéairement indépendante}
\end{description}
\paragraph{exemple}
$V=R^n$ une base de $R^n$
\[
e_1=
\begin{pmatrix}
1\cr
0\cr
...\cr
0\cr
\end{pmatrix}
e_2=
\begin{pmatrix}
0\cr
1\cr
...\cr
0\cr
\end{pmatrix}
e_n=
\begin{pmatrix}
0\cr
0\cr
...\cr
1\cr
\end{pmatrix}
\]
est une base qui s'appelle la base standard.
\newline
\[
v_1=
\begin{pmatrix}
1\cr
-1\cr
2\cr
\end{pmatrix}\]
est une base 
\newline
\[
v_1=
\begin{pmatrix}
1\cr
1\cr
2\cr
\end{pmatrix}
v_2=
\begin{pmatrix}
2\cr
2\cr
4\cr
\end{pmatrix}
\]
\newline
$B\{v_1,v_2\}$ n'est pas une base car linéairement indépendants

\subsection{rang et dimentions}
Soit $V\subset R^n$un sous-espace vectoriel
\newline
Soit B=$\{v_1,...v_r\}$ une base fixée
\begin{itemize}
 \item Vect$\{v_1,..,v_r\}=V$ Si $v\in V$
 \item B est libre $v=c_1v_i+...+c_rv_r$ (la combinaison linéaire est unique car c'est une base)
 \item $c_1,..,c_r$ sont les composantes de v dans la base B
\end{itemize}
\paragraph{exemple}
\[x=
\begin{pmatrix}
3\cr
12\cr
7\cr
\end{pmatrix}
B:v_1=
\begin{pmatrix}
3\cr
6\cr
2\cr
\end{pmatrix}
v_2=
\begin{pmatrix}
-1\cr
0\cr
1\cr
\end{pmatrix}
\]
\newline
alors les composantes de x qui appartient a B sont:
\[
\begin{pmatrix}
3&-1&|&3\cr
6&0&|&12\cr
2&1&|&7\cr
\end{pmatrix}
\equiv
\begin{pmatrix}
3&-1&3\cr
0&2&6\cr
0&0&0\cr
\end{pmatrix}
\]
\newline
il existe une solution: $c_2=3,c_1=2$
\newline
Ce qui implique
\newline
\[
x=2
\begin{pmatrix}
3\cr
6\cr
2\cr
\end{pmatrix}
+3
\begin{pmatrix}
-1\cr
0\cr
1\cr
\end{pmatrix}
\]
\subsubsection{dimention}
$V\subset R^n$ un sous-espace 
\newline
Si B est une base de V, B=$\{v_1,...,v_r\}$, on peut démontrer qu'une base quelconque B' de V possède exactement r vecteurs.
\paragraph{}
dimV=$|B|$
\newline
le nombre de vecteur de n'importe quelle base de B est nommé dimention du B.
\begin{description}
 \item n, nombre de colones:\\{ dim(Ker(A))+dim(Im(A))(théorème du rang)}
 \item rang de A:\\{la dimention de l'image de A}
\end{description}
\subsection{déterminants}
\[
\begin{pmatrix}
a_{11}&a{12}\cr
a_{21}&a_{22}\cr
\end{pmatrix}
\]
on a vu que A est inversible $\Leftrightarrow$ det(A):=$a_{11}a_{22}-a_{12}a_{21}$
\newline
pour que A soit inversible det(a)$\neq0$
\newline
$\vartriangle\neq 0\Leftrightarrow$ il y a deux positions de pivots dans une matrice 2x2
\newline
Mais nous povons élargire à une matrice 3x3
\[
\begin{pmatrix}
a_{11}&a_{12}&a_{13}\cr
a_{21}&a_{22}&a_{23}\cr
a_{31}&a_{32}&a_{33}\cr
\end{pmatrix}
\equiv
\begin{pmatrix}
a_{11}&a_{12}&a_{13}\cr
0&a_{11}a_{22}-a_{12}a_{21}&a_{11}a_{23}-a_{13}a_{21}\cr
0&a_{11}a_{32}-a{12}a_{31}&a_{11}a_{33}-a_{13}a_{31}\cr
\end{pmatrix}
\equiv
\begin{pmatrix}
a_{11}&*&*\cr
0&a_{11}a_{22}-a_{12}a_{21}&*\cr
0&0&a_{11}\vartriangle\cr
\end{pmatrix}
\]
\newline
$a_{11}\vartriangle\neq0\Leftrightarrow\vartriangle\neq0$
\paragraph{}
$a_{11}()-a_{12}()+a_{13}()=a_{11}det_1-a_{12}det_2+a_{13}det_3$
\subsubsection{définiton réccursive du déterminant}
soit A une matrice nxn
\newline
\[
\begin{pmatrix}
a_{11}&...&a_n\cr
...&...&...\cr
a_{n1}&...&a_{nn}\cr
\end{pmatrix}\]
\newline
$A_{ij}$ est la sous-matrice de A obtenue en supprimant la i-ème ligne et la i-ieme colonne.
\newline
$C_{ij}=(-1)^{i+j}det(A_{ij})$
\newline
$det(A)=a_{11}C_{11}+a_{12}C_{12}+..=a_{11}det(A_{11}+-...+...-..+.(-1)^{n+1}a_{1n}det(A_{1n}$
\subsubsection{théorème}
A est nxn, $A_{ij} cofacteur,C_{ij}=(-1)^{i+j}det(A_{ij})$
\newline
$\forall i$ det(A)=$a_{i1}C_{i1}+a_{i2}C_{i2}+...+a_{in}C_{in}$
\subsubsection{déterminant d'une matrice triangulaire}
\paragraph{matrice triangulaire suppérieure/inférieure?}
det(A)=$(-1)^{n+n}a_{nn}det(A_{nn})=a_{nn}det(A_{nn})=\prod_{i=1}^na_{ij}$
\subsubsection{propriété du déterminant}
comment calculer des déterminants à l'aide de la matrice de Gauss?
\begin{enumerate}
 \item remplacement: Si on ajoute à une ligne de A un multiple d'une autre ligne, on ne change pas le déterminant
 \item échange: on peut mais il faut changer le signe du déterminant à la fin.
 \item multiplication:  par un scalaire alors il faut multiplier le déterminant par la scalaire
\end{enumerate}

\paragraph{opéraiton sur les colonnes (théorème)}
Soit A nxn, $A^T,alors$ $det(A^T) = det(A)$

\paragraph{produit}
Si A,B sont nxn, det(AB)=det(A)det(B)
\newline
--------------------------------------------------------------------------------------
\subsubsection{Formule de Cramer}
Ax=b ou a est (nxn)
\paragraph{théorème}
Supposons que det(A)$\neq$ 0,alors la solution unique Ax=b est données par la formule suivante:
\newline
$x_i\frac{A_i(b)}{det(A)}\forall i=<,...,n$ ou $a_i(b)$ est obtenue à partir de A en remplaçant le i-ème colonne par le vecteur b.
\newline
$A_i(b)=[a_1,...,a_{i-1},[b],a_{i+1},...,a_n]$ 
\paragraph{}
preuve:
\newline
Soit I l amatrice identité nxn $I_i(x)$ une matrice nxn, alors $A_i(x)=[Ae_1*Ae_2*Ax*...*Ae_n]$=
\newline
\[
\begin{pmatrix}
a_1a_2...a_{i-1}ba_{i+1}...a_n\cr
\end{pmatrix}
\]
\paragraph{}
det (A)det($I_i(x)$)=det($A_i(b)$)
\newline
\[
\begin{pmatrix}
1&...&x_1&...\cr
...&1&x_2&...\cr
...&...&x_i&...\cr
...&...&...&...\cr
\end{pmatrix}
=x_i\]
\newline
$x_i$ det(A)=det($a_i(b)$) ce qui veut dire que $x_i=\frac{det(A_i(b))}{det(A)}$
\paragraph{attention}
cette méthode est très lente a faire tourner sur un ordi, il faut plutôt la voir comme une solution théorique!;)
\paragraph{exemple}
\begin{eqnarray}
 3x_1+x_2=10\cr
 x_1-x_2=2\cr
\end{eqnarray}
trouver la solution avec Cramer
\newline
\[
A=
\begin{pmatrix}
3&1\cr
1&-1\cr
\end{pmatrix}
b=
\begin{pmatrix}
10\cr
2\cr
\end{pmatrix}
\]
\newline
\[
A_i(b)=
\begin{pmatrix}
10&1\cr
2&-1\cr
\end{pmatrix}
A_2(b)=
\begin{pmatrix}
3&10\cr
1&2\cr
\end{pmatrix}
\]
\newline
det(A)=-4
\newline
det($A_1(b)$)=-12
\newline
det($A_2(b)$)=-4
\newline
alors $x_1=\frac{-12}{-4}=3,x_2=\frac{-4}{-4}=1$
\paragraph{inverse de A}
$A^{-1}=[u_1,u_2,...,u_n]*A=[Au_1,..,Au_n]=[e_1,...,e_n]$
\newline
$Au_j=e_j,u_j=
\begin{pmatrix}
x_{1j}\cr
...\cr
x_{ij}\cr
\end{pmatrix}
$
\newline
alors: $x_{ij}=\frac{des(A_i(e_j))}{det(A)}$
\paragraph{}
det($A_i(e_j)$)
\newline
$A_i(e_j)=
\begin{pmatrix}
a_1&...&a_{i-1}&b&a_{i+1}&...&a_n\cr
\end{pmatrix}
avec b=
\begin{pmatrix}
0\cr
0\cr
0\cr
1\cr
...\cr
\end{pmatrix}
$
\newline
det($A_i(e_j)$)=(-1)$^{j+1}det(A_{ij})$ : $x_{ij}=(-1)^{i+j}\frac{des A_{ji}}{det A}$
\begin{description}
 \item comm(A)=commatrice de A:\\{si A est inversible, comm(A)=$[(-1)^{i+j}det(A_{ij})]$}
\end{description}
Nous avons ainsi, $A^{-1}=\frac{1}{det(A)}Comm(A)^T$ méthode pour inverser les matrices 
-------------------------------------------------------------------------
\subsubsection{déterminants et géométrie}
$R^2$:
\newline
%schéma1
cas 1: $a_2 = ca_1 (colinearies)$ det(A)=0
\newline
%schéma2
cas 2: $a_1=[a,0], a_20[o,b]$ det(A)=ab
\newline
%schéma3
\paragraph{théorème}
L'aire du parallélogramme formé par $a_1,a_2$ est $|det(A)|,A=[a_1a_2]$
\newline
%schéma4
On transforme le parallélogramme en rectangle de manière fort astucieuse $A[a_1a_2]$ $A'=[a_1a_2']$
\newline
det(A)=det(A')  parce qu'on a seulement fait une opération de remplacement.
\paragraph{}
On peut ainsi conclure que la surface du rectangle =$|det(A')|=|det(A)|$
\paragraph{interprétation dans $R^3$}
c'est le volume du parallélépipède rectangle formé par les vecteurs
\paragraph{théorème transformations linéaires aire/volume}
%schéma5
comment lier aire de D avec l'aire de T(D)
\newline
Aire (T(D))=$|det(A)|=$ aire(D)
\newline
Nous pouvons faire des exemple très simple mais dont l'aire est difficile à calculer autrement:
\newline
elipse:=$\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$ si a=b=1 on a un cercle
\subsection{espaces vectoriels, sous-espaces}
Nous ne pouvons pas travailler avec des sous espaces de $R^n$ il nous faut voir une fonction comme une combinaison linéaire de fonction splus simple pour lesquelles on peut faire les transformations de Fourrier
\begin{itemize}
 \item traitement des signaux(sous-espace qi ne sont pas de $R^n$)
 \item théorie du contrôle
 \item procesus stochasitque (détermine le prix d'une option sur un titre)
\end{itemize}
\subsubsection{définitions générales}
\begin{description}
 \item espace vectoriel:\\{un ensemble V qui est non-vide et qui a deux opéraitons qui sont:(adidtion et multiplication par un scalaire)}
 \item addition:\\{v,w$\in V$ il existe v+w$\in V$}
 \item multiplicaiton par un scalaire:\\{$v\in R,r\in R$ alors, il existe $rv\in V$}
\end{description}
un espace vectoriel satisfait les propriétes suivantes:
\begin{enumerate}
 \item Si v et w $\in V$ alors $v+w\in V$ (addition vectorielle)
 \item u+v = v+u $\forall u,v\in V$ (commutativité)
 \item (u+v)+w=u+(v+w) $\forall v,w,u\in v$ (assoociativité)
 \item $\exists 0\in V$ tel que u+0=u $\forall u\in V$ (élément neutre)
 \item $\forall u\in V,\exists -u\in V/u+(-u)=0$ (il existe un inverse)
 \item $\forall c\in R, u\in v, cu\in V$ (multiplication par un scalaire)
 \item $\forall c\in R,u,v\in V$ $c(u+v)= cu +cv$ (distributivité)
 \item $\forall c,d\in R, u\in V$ $ (c+d)u=cu+du$ (distributivité)
 \item 1u = u $\forall u\in V$ (élément neutre pour la multiplication)
\end{enumerate}
Nous allons voir un exemple de sous-espace vectoriel qui n'est pas sous-espace de $R^n$:
\paragraph{exemple}
l'ensemble des suites réelles dounlement unfinies:
\newline
$V=\{(...,y_{-1},y_0,y_1,....):y_i\in R\forall i\in Z\}$
\newline
C'est un sous-espace vectoriel, il suffit de vérifier (lentment hahaha)
\newline
on prend:
\newline
$(..,y_{-1},y_0,y_1,...)$+$(...,z_{-1},z_0,z_{1},...)$
\newline
tu voyais comment prouver sans problèmes quand tu as écrit ces lignes, tu prends les deux vecteurs en haut et tu les additione/mutiplie sans trop de soucis XD
\newline
Attention de ne pas oublier de tout vérifier
\paragraph{exemple}
les suites réelles infinies
\newline
$V=\{(y_1,y_2,...):y_i\in R\}$
\newline
C'est aussi un espace vectoriel.
\paragraph{exemple}
Soit $P_n$ avec $n\in N, n\geq 1$
\newline
on défini $P_n$ comme un ensemble de polynomes:$\{a_0+a_1t+a_2t^2+...+a_nt^n:a_0,a_1,...,a_n\in R\}$
\newline
l'addition fonctionne, il y a un élément 0 (le polynome constant nul), multiplication du polynome par un scalaire
\newline
par contre si nous avions construit un polynome $Q_n$ avec $a_n\neq 0$ alors on aurait pas eu le polynome nul.
\paragraph{exemple}
$D\subset R^n$ un domaine
\newline
$V=\{f:D\mapsto R:$ f une fonction$\}$
\newline
On défnin une addition f,g$\subset V$ h=f+g, h:$D\rightarrow R$
\newline
$x\in D,$ h(x)=f(x)+g(x), l'addition en R est commutatif et associatif
\newline
il y a un élément nul: la fonction nulle avec $0\in R$
\newline
l'inverse de la fonction: -f(x)
\newline
multiplication par un scalaire: $\forall c\in R, f\in V$ (cf)(x)=cf(x)
\newline
distributivité sur R 
\paragraph{exemple de fonction}
$V_{pairs}\{f: R^n\rightarrow R, f(x)=f(-x)\forall x \in R\}$ est un espace vectoriel 
\newline
mais aussi: $V_{impair}\{f: R\rightarrow R: f(x)=-f(x),x  \in R\}$
\newline
(f+g)(-x)=+(f+g)(x)
\newline
f(-x)+g(-x)=-f(x)-g(x)=-(f+g)(x)
\newline
Ainsi, v pair/impair sont des sour espaces vectoriels
\subsubsection{théorème}
Soit V un espace vectoriel et soit $v_1,v_2,...v_r\in V, alors$ $Vect\{v_1,...,v_n\}\subset V$ est un sous-espace vectoriel
\newline
$Vect\{v_1,...,v_r\}=\{c_1v_1+...+c_rv_r:c_1,...;c_r\in R\}$
\newline
ce théorème nous permet de construire des sous-espaces vectoriels plus généraux
\subsection{notion de noyau et d'espace image: applicaitons linéaires}
Prennons un exemple dans le contexte des sous-espaces de $R^n$
\[
\begin{pmatrix}
1&3&-2&1\cr
0&1&0&-1\cr
1&1&0&1\cr
\end{pmatrix}
\]
\newline
sous forme écholonnée réduite augmentéé
\newline
\[
\begin{pmatrix}
1&0&0&2&0\cr
0&1&0&-1&0\cr
0&0&1&-1&0\cr
\end{pmatrix}
\]
\newline

Ker(A)=$\{x\in R^4:Ax=0\}$ sous espace de:$R$ qui est égal à $x_4(-2,1,1)$ que l'on calcule avec la matrice échelonnée réduite augmentéé avec le vecteur 0 à la fin.
\newline
Im(A)=$Vect\{a_1,...,a_4\}$ sous espace de: $R^3$ qui est égaleaux colonne de pivot de la matrice échelonné réduite:(1,0,0),(0,1,0),(0,0,1)
\paragraph{exemple}
\[A=
\begin{pmatrix}
1&0&-1&0&2&|&0\cr
0&1&0&0&-1&|&0\cr
0&0&0&1&-1&|&0\cr
0&0&0&0&0&|&0\cr
\end{pmatrix}
\]
\newline
nous cherchons ker(A): $\in R^2:(-2,1,0,1,1),(1,0,1,0,0)$ qui est sous-espce de $R^5$
\newline
espace image:(1,0,0,0), (0,1,0,0), (0,0,1,0) qui est sous-espace de $R^4$
\newline
nous pouvons voir que l'application n'est pas surjective car nous ne pouvons pas engendrer $R^4$ avec l'espace image: comment engendrer le vecteur:(0,0,0,1)?
\subsection{applications linéaires}
Nous allons oublier les matrice et utiliser que des applications linéaires.
\newline
Soit V,W deux applications linéaires
\begin{description}
 \item application linéaire $T:V\mapsto W$ si les propriétés sont respectées
\end{description}
\begin{itemize}
 \item $\forall v,w\in V, T(v+w)= T(v)+T(w)$
 \item $\forall c \subset R, v\in V, T(cv)=cT(v)$
\end{itemize}
\begin{description}
 \item noyau, Ker(T):\\{$v\in V, T(v)=0$}
 \item image, Im(T):\\{$w\in w/\exists v\in v / T(v)0w$}
\end{description}
\paragraph{théorème très facile mais néanmoins  important}
Ker(T) et Im(T) sont sous-espaces vectoriels respectivement de V et W.
\newline
Si v,w$\in$Ker(T)$\Rightarrow$ T(v+w)=T(v)+T(w)=0
\newline
$\Rightarrow v+w\in Ker(T9,T(cV)=cT(v)$
\newline
Si w',w'' $\in Im(T)$ alors $\exists v',v'' /T(v')=w'$ et $T(v'')=w''$
\newline
w'+w''=T(v')+T(v'')=T(v'+v'')
\newline
nous venous de prouver qu'ils sont sous-espaces vectoriels
\paragraph{exemple}
V espace des fonctions f:$[a,b]\rightarrow R$ qui sont des fonctions dérivables et continuement dérivables
\newline
W l'espace des fonctions g:$[a,b]\rightarrow R$ continues
\newline
Soit $V\rightarrow W$ D(f)=f'
\paragraph{théorème}
D est une application linéaire et Ker(D)= les fonctions constantes
\paragraph{preuve}
$\forall f,g\in V D(f+g)=D(f)+D(g)$ vrai car (f+g)'= f'+g'
\newline
$\forall c\in R,f\in V$ D(cf)=cD(f),(cf)'=cf'
\paragraph{}
Kef(D)= les fonctions constantes
\subsection{les familles Libres, Bases}
\subsubsection{Familles libres}
f=\{$v_1,...,v_r$\} est libre si:
\newline
$c_1v_1+...+c_rv_r=0\Rightarrow c_1=...=c_r=0$
\paragraph{exemple}
F=\{sint(t), cos(t)\} sin,cos:$R\rightarrow R$
\newline
Supposons que la famille F soit liée:
\newline
$\exists (c_1,c_2)\neq (0,0)/c_1sin(t)+c_2cos(t)=0$
\begin{itemize}
 \item si $c_1\neq0$ t=$\frac{pi}{2}\Rightarrow$ $c_1sin(\frac{pi}{2})+c_2cos(\frac{pi}{2})=c_1\neq0$
 \item si $c_2\neq0$ t=0$\Rightarrow c_1sin(0)+c_2sin(0)=c_2\neq 0$
\end{itemize}
Nous voyons la contradiction
\paragraph{exemple}
F=\{cos(2t),$\frac{1}{2}(cos^2(t)-sin^2(t))$\}
\newline
après développement nous pouvons voir que f(t)-2g(t)=0
\subsubsection{base}
\begin{description}
 \item une base:\\{Si V est un espace vectoriel, une famille F\{$v_1,...,v_r$\} est une base de V si F est libre et si V = à la parite engendrée par F(V=Vest\{F\})}
\end{description}
\paragraph{exemple}
$P_n=\{a_0+a_1t+...+a_nt^n;a_0,...,a_N\in R\}$
\newline
B=\{$1+t+t^2+..+t^n$\}
\paragraph{théorème de la base extraite}
Soit $V\subset R^n$ un sous-espace vectoriel et soit F=\{$v_1,...,v_r$\} une famille génératrice, alors V=Vect\{$v_1,..,v_r$\}
\newline
Alors il existe un sous-ensemble F'$\subset$ F qui est une base
\paragraph{bases de Ker(A),Im(A), exemples}
\[
\begin{pmatrix}
1&2&0\cr
0&1&-1\cr
0&-2&2\cr
\end{pmatrix}
\]
\newline
Il nous faut trouver une base our Im(A)
\[
A\sim
\begin{pmatrix}
1&0&-2\cr
0&1&-1\cr
0&0&0\cr
\end{pmatrix}
\]
\newline
Im(A)=(1,0,0),(0,1,0) qui est aussi la base.
\subsubsection{système de coordonnée}
Supposons que v est une espace et B\{$b_1,...,b_n$\} une base
\newline
$x\in V,\exists c_1,c_2,..,c_r\in R/x=c_1v_1+...+c_rv_r$
\paragraph{théorème}
$c_1,c_2,...,c_r\in R $ sont uniques
\paragraph{preuve}
Supposons que l'on puisse écrire x de deux manières:$c_1b_+...+c_rb_r=x=d_1b_1+...+d_rb_r$
\newline
$\Rightarrow (c_1-d_1)b_+...+(c_r-d_r)b_=0$
\newline
Par définition, la base est libre donc $c_1=d_1,...,c_r=d_r$
\newline
Par rapport à une base, nous avons les coordonnées d'un vecteur que l'on note $[x]_b=[c_1,...,c_r]$
\subsubsection{matrice de passage(changement de base)}
Cette matrice sert à transformer les vercteurs d'une base pour qu'ils soient exprimés selon une autre base
\paragraph{remarque sur l'image d'une matrice}
\[
A=
\begin{pmatrix}
1&2&0\cr
0&1&-1\cr
0&-2&2\cr
\end{pmatrix}
\]
\newline
Im(A)=Vect(1,0,0),(2,1,-2) attention, ce sont les colones de pivot qui nous donnent les colones qui engendrent l'espace image, mais ce ne sont pas les colones réduites qui engendrent
\newline
\paragraph{erreur que tu aurais faite}
il faut prendre les colones correspondantes à la matrice de pivot mais dans la matrice originale
\paragraph{exemple}
$P_3$=$\{a_0+a_1t+a_2t^2+a_3t^3\}:a_0,a_1,a_2,a_3\in R$
\newline
$B=\{1,t,t^2,t^3\}$
\newline
$P_3\mapsto R^4$
\newline
$p(t)\in P_3$
\newline
$p\mapsto [p]_B=
\begin{pmatrix}
 a_0\cr
 a_1\cr
 a_2\cr
 a_3\cr
\end{pmatrix}
\in R^4
$
\paragraph{}
si on prend comme exemple \{$1+t^2,2+t+3t^2,3+2t$\}
\newline
la famille est:
\[
\begin{pmatrix}
1\cr
0\cr
1\cr
0\cr
\end{pmatrix}
,
\begin{pmatrix}
2\cr
1\cr
3\cr
0\cr
\end{pmatrix}
,
\begin{pmatrix}
3\cr
2\cr
0\cr
0\cr
\end{pmatrix}
\]
\newline
la famille est libre car toutes les colonnes sont des colones de pivot
\subsection{dimention d'un espace vectoriel}
\paragraph{théorème}
Supposons qu'un espace vectoriel vect,V admet une base B=\{$b_1,..,b_n$\}. Alors toute famille de vecteurs de V contenant plus que n vecteurs est liée
\newline
preuve:
\newline
supposons que f$=\{u_1,...,u_r\}$ où $r>n$
\newline
$c_1u_1+...+c_ru_r=0$
\newline
$u_1\mapsto [u_1]_B\in R^n$
\newline
[$c_1u_1+...+c_ru_r]_B=c_1[u_1]_B+...+c_r[u_r]_B$
\newline
comme nous pouvons le voir quand nous faisons la matrice qui est nXr or, r est plus grand donc nous ne pourron pas obtenir des pivots dans toutes les colones,
\paragraph{théorème 2}
Si V admet une base B de n éléments, alors toutes les base de V contiennent exactement n éléments
\newline
démonstration:
\newline
Soit B' une base quelconque
\newline
Si $|B'|>|B|$, le théorème précédent implique que B' est liée
\newline
ce qui implique que $|B'|\leq |B|$ mais dans ce cas, selon le thm précédent, la famille B est liée
\newline
donc: $|B|=|B'|$
\subsection{définiton}
Soit V un espace vectoriel. S'il exste une famille gélératrice finie de V, on dit que V est de dimention finie.
 Alors la dimension de V est donnée par le nombre d'élément d'une base quelconque de V.
\newline
S'il n'y a pas de telle famille, nous disons que V est de dimension infinie.
\paragraph{exemple}
l'espace $P_n$
\newline
$dim(P_n)$= n+1
\subsubsection{théorème de la base incomplète}
Soit V un espace vectoriel de dimension finie et soit $H\subset V$ 
\newline
Toute famille libre de vecteurs de H peut-être complétée en une base de H.
\newline
Alors H est aussi de dimension finie et la dim(H) $\leq$ dim(V)
\paragraph{preuve}
$F=\{u_1,...,u_r\}$ une famille libre de H
\newline
Si F n'est pas génératrice, (Vect$ \{u_1,...,u_r\}\varsubsetneq H$)
\newline
Alors il existe $u_{r+1}<in H$, mais $u_{r+1}\ni Vect\{u_1,...,u_r\}$
\newline
Si nous considérons la famille $F'\{u_1,...,u_r,u_{r+1}\}$. Cette famille est libre
\newline
Et après on continue. En gros, après on ajoute jusqu'à arriver au moment que l'espace = H
\newline
le processus arrive à son terme quand V est de dimension finie
\subsubsection{deux caractéristique des bases}
\paragraph{fait 1}
Toute famille libre de V qui contient n vecteurs est une base
\newline
F est libre de n vect alors il existe une base B de V qui contient $F\subset B $ $\Rightarrow F=B$
\paragraph{fait 2}
Toute famille F génératrice de V avec $|F|=n$ forment une base. Grace au théorème de la base extraite($\exists B\subset F\Rightarrow F=B$)
\subsubsection{qq petits, tout petits thms}
\paragraph{théorème(dimension du noyau)}
On exprime le noyau en fonction des inconnues non-principales, il est agal au nombre de colones non principale
\paragraph{théorème(image)}
L'image est de dimension = au nombre de colones principales
\paragraph{définition}
le rang d'une matrice est égal à la dimension de l'espace image
\newline
\paragraph{définitions}
l'espace LgnA est l'espace engendré par les lignes de A qui sont non nulle dans la matrice échelonnées(pas réduite)
\paragraph{thm}
dim Im(A)= dim Lgn(A)
\newline
mais Lgn(A) est le nombre de lignes non nulle qui est toutjours exactement le nombre de pivots
\subsection{thm du rang}
nous rapellons la défintion du rang d'une matrice A que nous pouvons mnt définir avec la notion de dimension:
\begin{description}
 \item rang d'une matrice: \\{c'est la dimension de Im(A)}
\end{description}
\paragraph{théorème}
Soit A une matrice mxn
\begin{enumerate}
 \item rang(A)=dim(Lgn(A))
 \item dim(Ker(A))+rang(A)= n (le nombre de colonnes)
\end{enumerate}
Preuve:
\paragraph{1}
prenons une matrice échelonnée...
\newline
nous savons que dim(A)=\# de lignes de pivot=\#pivots de la matrice
\newline
On a calculé une base de Lgn(A) dim(Lgn(A))=\#nombre des lignes de pivot=\#nombre de pivots
\newline
On en déduit que rang A = dim(A)=Lgn(A)
\paragraph{2}
Ker(A)=\#colonnes non-principales et rang(A)=\#colonnes de pivot/colonnes principale.
\paragraph{exemple}
\[
A=
\begin{pmatrix}
 1&4&0&2&-1\cr
 3&12&1&5&5\cr
 2&8&1&3&2\cr
 5&20&2&8&8\cr
\end{pmatrix}
\sim
\begin{pmatrix}
 1&4&0&2&0\cr
 0&0&1&-1&0\cr
 0&0&0&0&1\cr
 0&0&0&0&0\cr
\end{pmatrix}
\]
\newline
Im(A)=une base est B=\{[1,3,2,5],[0,1,1,2],[-1,5,2,8]\}
\newline
dim(Im(A))=3
\paragraph{}
Lgn(A)=une base est donnée par B=\{[1,4,0,2,0],[0,0,1,-1,0],[0,0,0,0,1]\}
\newline
dim(Lgn(A))=3
\paragraph{}
ker(A), Ax=0, 
\begin{eqnarray}
 x_1=-4x_2-2x_4\cr
 x_3=x_4\cr
 x_5=0\cr
\end{eqnarray}
Ker(A)=$x_2[-4,1,0,0,0]+x_4[-2,0,1,1,0]$
\newline
Ainsi nous avons une base pour le noyau  B=\{[-4,1,0,0,0],[-2,0,1,1,0]\}
\newline
dim(Ker(A))=2
\newline
3+2=5(nous venons de vérifier le thm par un exemple)
\paragraph{Question}
Supposons que A soit une matrice 3x6, La dimension Ker(A), peut-être 2?
\newline
Si nous supposons que la dimention de Ker(A)=2 
\newline
dim(Ker(A))+rang(a)=6: alors rang(A)=4
\newline
$dim(Lgn(A))\leq 3$ car la matrice est engendrée par 3 vecteurs. 
\newline
alors l'affirmation est fausse car $rang(A)\leq 3$
\paragraph{autre question}
Pouvez-vous avoir dim(Ker(A))=3 pour une matrice 3x6
\newline
\[
\begin{pmatrix}
 1&0&0&0&0&0\cr
 0&1&0&0&0&0\cr
 0&0&1&0&0&0\cr
\end{pmatrix}
\]
\newline
dim(Ker(A))=3
\newline
dim(Im(A))=3
\newline
=dim(Lgn(A))=rang(A)
\subsection{théorème}
A est une matrice nxn avec les propriétés suivantes équavalentes:
\begin{enumerate}
 \item Les colonnes de A forment une base de $R^n$
 \item Les lignes de A forment un base de $R^n$
 \item L'image de A=n
 \item rang(A)=n
 \item dim(Ker(A)=0
 \item Ker(A)=\{0\}
\end{enumerate}
ssi A est inversible bien sûr
\paragraph{exemple}
\[
A=
\begin{pmatrix}
 0&...&0\cr
 ...&...&...\cr
 0&...&0\cr
\end{pmatrix}
\]
\newline
Ker(A)=$R^n$ $\Leftrightarrow$ dim(Ker(A))=n
\newline
dim(Im(A))=0 $\Rightarrow$ Im(a)=\{0\}$\Rightarrow$rang(A)=0
\subsection{changement de base}
V un espace vectoriel de dimension m qui est fini
\newline
$V\rightarrow R^n$ $x\in V\rightarrow[x]_B$
\newline
Supposons que C est une autre base.(définition du vecteur: élément d0un espace vectoriel)
\newline
on cherche une transformation de $[x]_b$ dans $[x]_C$
\paragraph{exemple}
Soit B=\{$b_1,b_2$\} et C=\{$c_1,c_2$\} de $R^2$
\newline
Soit V un espace de dimension V=2
\newline
soit B,C des bases de V
\begin{itemize}
 \item $b_1=3c_1+c_2$
 \item $b_2=-c_1+c_2$
\end{itemize}

$x\in V,x=2b_1+b_2$
\newline
$[x]_B=[2,1]$
\newline
$[x]_C=[2b_1+b_2]=2 [b_1]_C+[b_2]_C=[[b_1]_C[b_2]_c][2,1]$ avec $[[b_1]_C[b_2]_c]$ la matrice de passage qui nous permet de passer de B à C ( on écrit $B\leftarrow C$)
\newline
Dans notre cas la marice de passage est\[
\begin{pmatrix}
 3&-1\cr
 1&1\cr
\end{pmatrix}
\begin{pmatrix}
 2\cr
 1\cr
\end{pmatrix}
=
\begin{pmatrix}
 5\cr
 3\cr
\end{pmatrix}
\]
\subsubsection{théorème}
soit B=\{$b_1,..,b_n$\} et C=\{$c_1,..,c_n$\} deux bases d'un espace vectoriel V, dim(V)=n
\newline
Il existe une matrice unique ($C\leftarrow B$)P tel que $[x]_C=P[x]_B$
\newline
La matrice $P_{C\leftarrow B}=[[b_1]_C[b_2]_C...[b_n]_C]$
\paragraph
la matrice dont les colonnes sont les coordonnées des vecteurs de B par rapport à la base C
\paragraph{à noter}
$P_{C\longleftarrow B}$ est inversible nous pouvons écrire: $[b_i]_B=P_{C\longleftarrow B}[b_i]_C$ $\forall i=1,2,...,n$
\newline
Nous pouvons voitr que $P_{B\longleftarrow C}=P^{-1}_{C\longleftarrow B}$
\paragraph{demonstration du thm}
$x\in B\rightarrow[x]_B$ $x=u_1b_1+...+u_nb_n$
\newline
$\downarrow$
\newline
$[x]_C$ sachant que $[x]_C$=$v_1[b_1]_C+...+u_n[b_n]_C$
\newline
$P_{C\leftarrow B}=[b_1]C...[b_n]_C*[u_1..u_n](=[x]_B)$
\subsubsection{changments de base dans $R^n$}
$P_{\varepsilon\leftarrow B}=[b_1...b_n]$
\newline
Soit $\varepsilon$= la base canonique =\{[1,0,....],[0,1,....],...,[...,0,1]\}
\newline
Si tu as des pbs dans les exercices regrarde dans le livre il y a pleins d'exemples


\section{valeur ropres et vecteurs propres}
le but est de comprendre une application en la décomposant en partie plus simples 
\begin{description}
 \item valeur propre:\\{si l'applicaiton matricielle $Ax=\lambda x$ possède un solution non triviale, avec $\lambda$ une valeur propre}
\end{description}
\paragraph{exemple}
\[
Av=
\begin{pmatrix}
3&-2\cr
1&0\cr
\end{pmatrix}
\begin{pmatrix}
2\cr
1\cr
\end{pmatrix}
=
\begin{pmatrix}
4\cr
2\cr
\end{pmatrix}
=2
\begin{pmatrix}
2\cr
1\cr
\end{pmatrix}
=2v
\]
on peut alors dire que Ax=2x
\paragraph{exemple}
$\lambda=3$
\newline
\[
\begin{pmatrix}
5&2\cr
3&6\cr
\end{pmatrix}
\]
\newline
probleme: trouver $x\in R^2/Ax=3x$
\newline
nosu faisons comme cela: Ax-3x=0
\newline
(A-1I)x=0
\newline
ce qui nous donne:
\[
\begin{pmatrix}
2&2&|&0\cr
3&3&|&0\cr
\end{pmatrix}
\sim
\begin{pmatrix}
 1&1&|&0\cr
 0&0&|&0\cr
\end{pmatrix}
\]
\newline
ce qui nous dit que $x_1+x_2=0$ 
\newline
\[
x_2
\begin{pmatrix}
 1\cr
 -1\cr
\end{pmatrix}
\]
\paragraph{exemple}
$\lambda = 5$
\newline
\[
A=
\begin{pmatrix}
6&-3&1\cr
3&0&5\cr
2&2&6\cr
\end{pmatrix}
\]
\newline
$\lambda=5$ est-elle une valeur propre?
\newline
Ax=5x
\newline
8A-5I)x=0
\newline
\[
\begin{pmatrix}
1&-3&1&|&0\cr
3&-5&5&|&0\cr
2&2&1&|&0\cr
\end{pmatrix}
\sim
\begin{pmatrix}
1&-3&1&0\cr
0&4&2&0\cr
0&0&-5&0\cr
\end{pmatrix}
\]
\newline
avec la forme échelonnée, que la valeur 5 n'est pas une valeur propre car x=0(solution triviale)
\subsubsection{vecteurs propres}
\begin{description}
 \item vecteur propre:\\{un vecteur x tel que $Ax=\lambda x$ est un vecteur propre pour $\lambda$}
\end{description}
\paragraph{remarque}
les vecteurs propres correspondants à une valeur propre donnée, forment un sous-espace vectoriel
\subsubsection{théorème}
Les valeurs propres d'une matrice triangulaire sont les coéficients de sa diagonale principale
\newline
Nous venons de prouver au tableau pour la matrice diagonale (tu peux le voir en prenant une matrice avec des $a_{nn}$ dans la diago)
\paragraph{cas général}
A est inversible $\Leftrightarrow \lambda = 0$ n'est pas une valeur propre
\paragraph{preuve}
$\lambda=0$ une valeur propre $\Leftrightarrow\exists x\neq0/Ax=0,Ker(A)\neq 0\Leftrightarrow A n'est pas inversible$
\subsubsection{théorème}
Soit A nxn
\newline
$v_1,..,v_r$ vecteur propre de valeur propre $\lambda_1,...,\lambda_r$ distincttes $\Rightarrow$ F=\{$v_1,..,v_r$\} est libre
\paragraph{preuve}
supposons le contraire:
\newline
Il existe $v_i=c_1v_1+...+cv_{i-1}$ et que 2 est minimale
\newline
alors: \{$v_1,...,v_{i-1}$\} est libre
\newline
$Av_i=c_1Av_1+...+c_{i-1}Av_{i-1}= \lambda_1c_1v_1+...+\lambda_1c_{i-1}v_ {i-1}$
\newline
$c_1(\lambda_i-\lambda_1)v_1+...+c_i(\lambda_i-\lambda_{i-1})v_{i-1}=0$
\newline
au moins un $c_1(\lambda_i-\lambda_j)\neq 0)$ ce qui est, un énnorme contradiction.
\subsubsection{calcul des valeurs propres}
\paragraph{exemple}
A=
\[
\begin{pmatrix}
5&2\cr
3&6\cr
\end{pmatrix}
\]
\newline
on cherche les valeurs propres
\newline
on cherche $\lambda/(A-\lambda I)x=0$ a une solution non-triviale
\newline
$\lambda$ est une valeur propre ssi B$(A-\lambda I)$ est non-inversible ssi det(B)=0
\paragraph
on va extraire une équation pour exprimer les différents $\lambda$
\newline
\[
A-\lambda I=
\begin{pmatrix}
5-\lambda&2\cr
3&6-\lambda\cr
\end{pmatrix}
\]
det(B)=0$\Leftrightarrow$ $\lambda^2-11\lambda+24=0$ on trouve $\lambda_1=8$ et $\lambda_2=3$
\paragraph{en général, théorème}
$\lambda$ est une valeur propre de A si et seulement si ($Ax=\lambda x$ a une solution non-triviale) det($A-\lambda I$)=0
\subsection{exemple pour équation caractéristque}
\[
A=
\begin{pmatrix}
3&-1&1&0\cr
0&2&1&5\cr
0&0&-1&-2\cr
0&0&0&-1\cr
\end{pmatrix}
\]
\newline
calculer l'équation caractéristique
\newline
$A-\lambda I$
\[
\begin{pmatrix}
3-\lambda &-1&1&0\cr
0&2-\lambda &1&5\cr
0&0&-1-\lambda&-2\cr
0&0&0&-1-\lambda\cr
\end{pmatrix}
\]
\newline
det($A-\lambda I$)=$(\lambda^2-2\lambda+1)(\lambda^2-5\lambda+6)=0$
\newline
les valeurs propres sont:3,2,-1(multiplicité 2)
\subsection{notion de matrices semblables}
\begin{description}
 \item A et B semblables(nxn):\\{s'il existe une matrice P(nxn) inversible telle que $B =P^{-1}AP$}
\end{description}
\paragraph{théorème}
Si A et B sont semblables, alors elles ont le même polynome caractéristique.
\subsubsection{Vecteurs propres et applications linéaires}
t:$V(R^n)\to W(R^m)$ une application linéaire A=$P\cdot D\cdot P^{-1}$
\newline
B une base de V
\newline
C une base de l'espace d'arrivée
\begin{itemize}
 \item $[x]_B$
 \item $[T(x)]_C$
\end{itemize}
x=$r_1b_1+r_2b_2+...+r_nb_n$ o uB\{$b_1,...,b_n$\}
\newline
T(x)=$r_1T(B_1)+...+r_nT(b_n)\in W$
\paragraph{}
On calcule $[T(x)]_C=r_1[T(b_1)]_C+...+r_n[T(b_n)]_C=M[x]_B$
\newline
ou
\newline
\[
M=[[T(b_1)]_C...[T(b_n)]_C]
\]
\subsubsection{exemple}
Supposons T$:V\to W$
\newline
dim V=2, B=\{$b_1,b_2$\}
\newline
dim W=3, C=\{$c_1,c_2,c_3$\}
\newline
$T(b_1)=2c_1-c_3+3c_3$
\newline
$T(b_2)=c_1+c_2-2c_3$
\newline
alors 
\newline
\[
\begin{pmatrix}
 2&1\cr
 -1&1\cr
 3&-2\cr
\end{pmatrix}
\]
\subsection{endomorphismes d'un espace vectoriel}
W=V: T:$V\to V,\forall x\in V$
\newline
e=B
\newline
$[T(x)]_B=[T]_B[x]_B$
\newline
avec [T] une matrice nxn
\paragraph{exemple}
$P_3\to P_3$ f(x)=$a_0+a_1x+a_2x^2+a_3x^3\to 2a_2+6a_3x$=f''(x)
\newline
On a les bases standart de $P_2$ et $P_3$ \{$1,t,t^2$\} et \{$1,t,t^2,t^3$\}
\newline
$[D(f)]_{B2}=[D]_{B2}[f]_{B2}$
\begin{itemize}
 \item D(1)=0
 \item D(t)=1
 \item D(t)=2t
\end{itemize}
\[
[D]_{B2}
\begin{pmatrix}
 0&1&0\cr
 0&0&2\cr
 0&0&0\cr
\end{pmatrix}
\]
\newline
pour trouver la matrice on fait:
\newline
\[
D(\begin{pmatrix}
   1\cr
   0\cr
   0\cr
  \end{pmatrix}
)
=\begin{pmatrix}
  0\cr
  0\cr
  0\cr
 \end{pmatrix}
\]
\newline
et on le fait avec tous les éléments de la base B
\subsubsection{endomorphismes de $R^n$}
\paragraph{théorème}
Supposons que A soit nxn siagonalisable tel que $A=P\cdot D\cdot P^{-1}$
\newline
Alors si B est la base donnée par les colonnes de P alors (la matrice de A au cas ou tu serais trop con pour savoir la notation)$[A]_B=D$
\newline
$A=P\cdot D\cdot P^{-1}, P=[b_1,...,b_n]$
\newline
$[A]_B=D, [T]_B=[[T(b_1)]_B...[T(b_n)]_B]=[[Ab_1]_B...[Ab_n]_B]$
\newline
$PD=AP=[Ab_1...ab_n]$
\paragraph{question}
les colones de PD dans la base B?
\paragraph{réponse}
les colonnes de PD:$[PD]_B=[diagonale (x_1,...,x_n)]$ car $P^{-1}$ est la matrice de passage
\newline
$x\to Ax$ 
\newline
$\downarrow(P^{-1})$ $\uparrow(P)$
\newline
$[x]_B\to(D) [Ax]_B$
\paragraph{exemple}
\[
A=
\begin{pmatrix}
 4&-9\cr
 4&-8\cr
\end{pmatrix}
\]
\newline
polynome caractéristique: $P_A(\lambda)=det\begin{pmatrix}
                                            4-\lambda&-9\cr
                                            4&-8-\lambda\cr
                                           \end{pmatrix}$
\newline
$=(\lambda -4)(\lambda-8)+36=...
$
\newline
$\lambda =-2$ est une valeur propre de multi. =2
\newline
\[
  A+ 2I
  \begin{pmatrix}
   6&-9\cr
   4&-6\cr
  \end{pmatrix}
\]
\newline
si nous calculons le noyau de l'application $Ker(A+ 2I)=Vect\{\begin{pmatrix}
                                                                   3\cr
                                                                   2\cr
                                                                  \end{pmatrix}
\}$
\newline
nous voyons que la matrice n'est pas diagonalisable, mais nous voulons donner à la matrice une forme suffisemment simple/proche d'une diagonale...
\newline
la question est: comment l'obtenir?
\paragraph{}
il faut calculer le $Ker((A+2I)^2)$:
\newline
\[
=
\begin{pmatrix}
 0&0\cr
 0&0\cr
\end{pmatrix}
\]
\newline
le $Ker(A+2I)^2=R^2$
\newline
$Ker(A+2I)=Vect\{\begin{pmatrix}
                  3\cr
                  2\cr
                 \end{pmatrix}
                 \}
\subset Ker(A+2I)^2
$
\newline
Il faut compléter la base 3,2 à un base de $R^2$
\newline
$b_1=[3,2],b_2=[2,1]$ une base de $Ker(A+2I)^2$
\newline
\[
\begin{pmatrix}
 3&2\cr
 2&1\cr
\end{pmatrix}
\]
\newline
$P^{-1}AP=[T]_B$
\newline
\[
A=
\begin{pmatrix}
 -2&1\cr
 0&-2\cr
\end{pmatrix}
\]
\newline
nous avons choisi $b_1$ pour que la forme soit plus jolie et plus facile à utilise dans les calculs
\newline
nous appelons cette matrice: la forme canonique de Jordan
\subsubsection{nouvel exemple}
p(x)=$x^2+4$ pas factorisablle sur R
\newline
\[
\begin{pmatrix}
 0&-2\cr
 2&0\cr
\end{pmatrix}
P_A(\lambda)=
\begin{pmatrix}
 -\lambda&-2\cr
 2&-\lambda\cr
\end{pmatrix}
=\lambda^2+4
\]
\newline
sur C: $\lambda^2+4=0: \lambda=+-2i$
\newline
Ker(A+-2iI)=
\newline
\[
Ker(\begin{pmatrix}
     +-2i&-2\cr
     2&+-2i\cr
    \end{pmatrix}
)
\]
\newline
Nous allons faire les calculs
\newline
\[
Ker\begin{pmatrix}
    2i&-2\cr
    2&2i\cr
   \end{pmatrix}
\]
\newline
après élimination de Gauss; 
\newline
\[
\begin{pmatrix}
 x_1\cr
 x_2\cr
\end{pmatrix}
=
\begin{pmatrix}
 i\cr
 -1\cr
\end{pmatrix}
\]
\newline
$Vect_C\{i,1\}$ sur C
\newline
\[
\begin{pmatrix}
 0&-2\cr
 2&0\cr
\end{pmatrix}
\begin{pmatrix}
 1\cr
 i\cr
\end{pmatrix}
=
\begin{pmatrix}
 -2i\cr
 2\cr
\end{pmatrix}
=-2i(\lambda)
\begin{pmatrix}
 1\cr
 i\cr
\end{pmatrix}
\]
\paragraph{exemple}
\[
A=
\begin{pmatrix}
 cos\theta&sin\theta\cr
 -sim\theta&cos\theta
\end{pmatrix}
\]
\newline
polynome caractéristique:
\newline
$P_A(\lambda)=det[A]-\lambda I$ = $\lambda^2-2cos\theta\lambda+1$
\newline
$\theta\neq pi\cdot n$ alors les racines sont complexes mais pas réelles
\newline
$\lambda_{1,2}=cos\theta+-isin\theta$
\paragraph
On est interessé par la compréhention de l'action d'une matrice A 2x2 sont les racines sont complexes.
\newline
\[
A=
\begin{pmatrix}
 a&-b\cr
 b&a\cr
 \end{pmatrix}
 a,b\in R
\]
\newline
det A=$a^2+b^2$
\newline
det A=0$\Leftrightarrow a=b=0$
\newline
\[
\begin{pmatrix}
 0&-2\cr
 2&0\cr
\end{pmatrix}
a=0,b=2
\]
\newline
le polynome caractéristique est: $P_A(\lambda)=(\lambda-a)^2+b^2$ (nous pouvons voir qu'il y a un lien avec la matrice de rotation mais on ne voit pas bien quoi)
\newline
$\lambda_{1,2}=a+-ib=r(cos\theta+-isin\theta)$
\newline
$r=\sqrt{a^2+b^2}$ $cos\theta=\frac{a}{\sqrt{a^2+b^2}},sin\theta=\frac{b}{\sqrt{a^2+b^2}}$
\newline
\[
A=
\begin{pmatrix}
 r&0\cr
 0&r\cr
\end{pmatrix}
\begin{pmatrix}
 cos\theta&-sin\theta\cr
 sin\theta&cos\theta\cr
\end{pmatrix}
\]
\subsubsection{Action d'une matrice 2x2 dont les racines du polynome caractéristique sont complèxes}
\paragraph{exemple}
\[
\begin{pmatrix}
 0.5&-0.6\cr
 0.15&1.1\cr
\end{pmatrix}
\]
\newline
$\lambda_{1,2}=o.8+-0.6i:(0.5-\lambda)(\lambda-1.1)+0.75\cdot 0.6$
\newline
$=\lambda^2-1.6\lambda+1$
\newline
nous allons assayer de calculer le vecteur propre correspondant, dans ce cas nous devons faire une observation: l'une des valeur propre est le conjugué de l'autre (c'est logique car le +- i ;))
\newline
$Av=\lambda v,\lambda\in C, A$ une matrice dont les éléments sont réels
\newline
$(Av)bar=A\cdot(v)bar$
\newline
$(\lambda v)bar=(\lambda)bar(v)bar$
\newline
nous avons comme vecteur propre: $(\lambda)bar(v)bar$ comme vecteur propres
\paragraph
calcul du'un vecteur propre pour $\lambda=0.8+-0.6i$
\newline
\[
A-\lambda I=
\begin{pmatrix}
 0.5-(0.8+i\cdot 0.6)&-0.6\cr
 0.75&1.1(0.8+i\cdot 0.6)\cr
\end{pmatrix}
\]
\newline
$Ker(a-\lambda I)$
\newline
\[
\begin{pmatrix}
 0.45&0.18-0.36i&0\cr
 0&0&0\cr
\end{pmatrix}
v_1=
\begin{pmatrix}
 \frac{-2-4i}{5}\cr
 1\cr
\end{pmatrix}
\]
\newline
$0.45x_1+(0.18-0.36i)x_2=0$
\newline
$x_1=\frac{-2-4i}{5}x_2$
\paragraph
on sait que $v_2=[\frac{-2-4i}{5},1]$ est un vecteur propre pour $\lambda_2=0.8-i\cdot 0.6$
\newline
P=$[Re(v_1) Im(v_1)]
=
\begin{pmatrix}
 -\frac{2}{5}&\frac{4}{5}\cr
 1&0\cr
\end{pmatrix}
$
\newline
On va calculer $P^{-1}AP$
\paragraph
Q=[$v_1(v_1)bar$] $Q^{-1}AQ=
\begin{pmatrix}
 \lambda_1&0\cr
 0&\lambda_2\cr
\end{pmatrix}
$
\subsubsection{théorème}
Soit A une matrice nxn. Supposons que $\{v_1,...,v_n\}$ soit une matrice libre de vecteurs propres de A dont les vecteurs propres
 sont $\lambda_1,...,\lambda_n$. Alors la matrice $D=P^{-1}AP$ est la matrice diagonale:
 $\begin{pmatrix}
   \lambda_1&...&...\cr
   ...&...&\cr
   ...&...&\lambda_n\cr
  \end{pmatrix}
$
\paragraph{preuve}
AP=$A[v_1...v_n]=[Av_1...Av_n]=\lambda_1v_1...\lambda_nv_n]$
\newline
$\Rightarrow AP=PD\Rightarrow D=P^{-1}AP$
\newline
avec d la matrice diagonale des vecteurs propres.
\subsubsection{remarque annexe}
on peut effectuer une réduciton de Gauss avec des nombre complexes, il suffit de faire les même opérations, avec des nombres complexes
\subsubsection{thm de pythagore (généralisé) }
Si u, v tel que uv=0
\newline
$||u+v||^2=||u||^2+||v||^2$
\paragraph{généralisation}
On supprime la condition uv=o
\newline
alors $||u+v||^2=||u||^2+||v||^2+2||u||||v||$
\newline
on démontre avec la trigonoométrie (thm du cosinus: $c^2=a^2+b^2cos(\gamma)$
\paragraph{démonstration}
Si $\gamma=\frac{\pi}{2}\Rightarrow c^2=a^2+b^2$
\newline
$cos(\gamma)=cos(\pi-\gamma)$
\newline
$||u+v||^2=||u||^2+||v||^2-2||u||||v||cos(\pi-\gamma)$
\newline
$=||u||^2+||v||^2+2||u||||v||cos(\gamma)$
\subsubsection{orthogonalité d'un sous-vecteur}
Soit $W\subset R^n$ un sous-espace
\newline
$W^{\bot}=\{z\in R^n:\forall w\in W,zw=0\}$
\newline
1)Soit $W^{\bot}=$ la droite qui passe par l'origine orthogonale à W
%schéma1
\newline
2)W= une droite qui passe par l'origine
\newline
Soit $W^{\bot}=$ le plan orthogonal à W
\newline
3)$(W^{\bot})^{\bot}=W$
\newline
$W\cap W^{\bot}=\{0\}$
\newline
Soit $v\in W\cap W^{\bot}\Rightarrow u(\in W)\cdot v(\in W^{\bot})=0\Rightarrow v=0$
\subsubsection{théorème}
Soit A une matrice mxn
\begin{enumerate}
 \item Ker(A)=Lgn(A)
 \item $(Im(A))^{\bot}=Ker(A^T)$
\end{enumerate}
\subsubsection{familles orthogonales}
\[e_1
\begin{pmatrix}
 1\cr
 0\cr
 0\cr
\end{pmatrix}, e_2
\begin{pmatrix}
 0\cr
 1\cr
 0\cr
\end{pmatrix}, e_3
\begin{pmatrix}
 0\cr
 0\cr
 1\cr
\end{pmatrix}
\]
\newline
$e_i\cdot e_j:1$ si $i=j$, 0 sinon
\begin{description}
 \item une famille orthogonale:\\{Une famille $F=\{u_1,...,u_k\}$ est othogonale si $\forall u_iu_j=0$}
 \item une base orthogonale:\\{une bas d'un espace V qui est aussi une famille orthogonale}
 \end{description}
\paragraph{exemple}
$F=\{u_1,u_2\}$ est une base orthogonale de $R^2$
\newline
\[
u_1=
\begin{pmatrix}
 1\cr
 -1\cr
\end{pmatrix}
,u_2=
\begin{pmatrix}
 1\cr
 1\cr
\end{pmatrix}
\]
\newline
orthogonale car: 1+(-1)=0
\paragraph{coordonnées}
Soit $F=\{u_1,...,u_n\}$ une base orthogonale de $R^n$ et soit $y\in R^n$
\newline
$y=c_1u_1+...+c_nu_n,c_1,...,c_n\in R$
\newline
$y\cdot u_i=(c_1u_1+...+c_nu_n,c_1,...,c_n)u_i=c_i(u_1u_i)$
\newline
$c_i=\frac{yu_i}{u_iu_i},\forall i$
\paragraph{exemple}
\[
u_1=
\begin{pmatrix}
 1\cr
 -1\cr
\end{pmatrix}, u_2=
\begin{pmatrix}
 1\cr
 1\cr
\end{pmatrix}, y=
\begin{pmatrix}
 2\cr
 1\cr
\end{pmatrix}
\]
\newline
nous voulons calculer les coordonnées de y par rapport à $\{u_1,u_2\}$
\newline
$c_1=\frac{yu_1}{u_1u_1}=\frac{1}{2}$
\newline
$c_2=\frac{yu_2}{u_2u_2}=\frac{3}{2}$
\newline
\[
\frac{1}{2}
\begin{pmatrix}
 1\cr
 -1\cr
\end{pmatrix}
+\frac{3}{2}
\begin{pmatrix}
 1\cr
 1\cr
\end{pmatrix}
\]
\paragraph{projection orthogonale}
y' la projection orthogonale de y sur Vect\{u\}
%schéma2
\newline
On note que (y-y')$\bot$u
\newline
$\Rightarrow$(y-y')u=0
\newline
Mais nous pouvons voir que y'=cu
\newline
y-cu)u=0
\newline
$c=\frac{yu}{uu}\Rightarrow$y'=$\frac{yu}{uu}u$
\paragraph{exemple}
%shéma3
\subsubsection{Familles orthonormées}
$F=\{u_1...u_n\}$ est une famille orthonormée si 
\begin{enumerate}
 \item F est orthogonale
 \item $\forall i,||u_i||=1$($u_i$ est unitaire)
\end{enumerate}
\subsubsection{théorème}
Toute famille orthogonale est libre
\paragraph{preuve}
supposons que $F=\{u_1,...,u_n\}$ est orthogonale ($c_1u_1+...c_nu_n=0$), $c_i\neq 0$
\newline
et supposons qu'elle soit libre
\newline
0=$u_i(c_1u_1+...+c_nu_n)=c_i(u_iu_i)$
\newline
or $u_iu_i\neq 0$
\newline
$\Rightarrow c_i=0:$ contradiction
\subsubsection{théorème}
Soit U une matrice mxn dont les colonnes forment une famille orthonormée.($m\geq n$)
\begin{enumerate}
 \item $||Ux||=||x||$ (les distance sont préservées)
 \item (Ux)(Uy)=xy,$\forall x,y\in R^n$
 \item $(Ux)(Uy)=0 \Leftrightarrow xy=0$
\end{enumerate}
\paragraph{preuve 1}
$||Ux||^2=(Ux)(Ux)=(Ux)^T(Ux)=x^TU^TUx=x^Tx=||x||^2$
\paragraph{preuve 2}
$(Ux)(Uy)=(Ux)^T(Uy)=x^TU^TUy=xy$
\paragraph{preuve 3}
$(Ux)(Uy)=xy$
\subsubsection{projection orthogonales}
%schéma4
Supposons que $W\subset R^n $soit un sous-espace
\newline
$y\in R^n$
\newline
nous cherchons à la définition de la projeciton W(y)
\newline
y'=proj W(y)
\newline
y-y' est orthogonal à W.
\newline
Supposons que $F=\{u_1,...,u_k\}$ soit une base orthogonale de W: dim(W)=k et $W\subset R^n$
\newline
$y'=u_1y_1+...+u_ky_k$ tel que $(y-y')\bot W$
\newline
$\Leftrightarrow (y-y')\cdot u_i=0,\forall i=1,2,...,k$
\newline
$yu_i=y'u_i =c_iu_iu_i, \forall i, c_i=\frac{y'u_i}{u_iu_i}$
\paragraph{définition}
On définit (étant donné une base orthogonale de W $\{u_1,...,u_k\}$
\newline
proj W(y)=$\frac{yu_1}{u_1u_1}u_i+...+\frac{yu_k}{u_ku_k}u_k$
\newline
-------------------------------------------
\newline
Soit A nxr
\newline
\[
\begin{pmatrix}
 3&2\cr
 1&0\cr
 0&1\cr
 0&1\cr
\end{pmatrix}
,b=\begin{pmatrix}
    2\cr
    1\cr
    1\cr
    -1\cr
   \end{pmatrix}
\]
\newline
soit $b\in R^r$
\newline
Il nous intéresse de trouver une approximation de b comme une combinaison linéaire des colonnes de A.
\newline
on cherche une droite qui représente une approximation de tous les points, on cherche un model linéaire: $y=\beta_0+\beta_1x$
\newline
on peut mesurer la qualité de l'approximation par la distance
\newline
$||y-A\beta||$ ou $\beta=\begin{pmatrix}
                          \beta_0\cr
                          \beta_1\cr
                         \end{pmatrix}$
\newline
Problème: trouver une approximation de y comme une combinaison linéaire des colonnes de A.
\newline
nous avons comme condition pour l'orthogonalité: $A^T(b-b^{chapeau})$ sachant que $b^{chapeau}$ est le vecteur orthogonal a Ax
\newline
nous pouvons donc dire: $A^T(b-A\beta)=0\Rightarrow A^Tb=A^TA\beta$
\newline
$(A^TA)^{-1}=
\begin{pmatrix}
 1&1&1&1\cr
 2&3&4\cr
\end{pmatrix}
\begin{pmatrix}
 1&2\cr
 1&3\cr
 1&1\cr
 1&4\cr
\end{pmatrix}
=\begin{pmatrix}
  4&10\cr
  10&30\cr
 \end{pmatrix}
$
\newline
\[
\Rightarrow \beta=\begin{pmatrix}
                   4&10\cr
                   10&30\cr
                  \end{pmatrix}^{-1}
\]
\newline
\[
\begin{pmatrix}
 1&1&1&1\cr
 2&3&1&4\cr
\end{pmatrix}
\begin{pmatrix}
 -1\cr
 1\cr
 1\cr
 -1\cr
\end{pmatrix}(vrai_valeurs)
=\begin{pmatrix}
  0\cr
  -2\cr
 \end{pmatrix}
\]
\newline
la droite $\beta_0+\beta_1x$ se nomme la droite de régression
\paragraph{exemple}
\[
y=\begin{pmatrix}
   3\cr
   5\cr
   7\cr
   -3\cr
  \end{pmatrix}
  x=\begin{pmatrix}
     1&3&5\cr
     1&1&0\cr
     1&1&2\cr
     1&3&3\cr
    \end{pmatrix}
\]
\newline
sachant que 
\[x_1=\begin{pmatrix}
       3\cr
       1\cr
       1\cr
       3\cr
      \end{pmatrix},x_2
      \begin{pmatrix}
       5\cr
       0\cr
       2\cr
       3\cr
      \end{pmatrix}
\]
\newline
$y\sim \beta_0+\beta_1x_1+\beta_2x_2$ deux variables indépendantes (facteurs)
\newline
ou 
\[
\beta=\begin{pmatrix}
       \beta_0\cr
       \beta_1\cr
       \beta_2\cr
      \end{pmatrix}
, x\beta=\begin{pmatrix}
          \beta_0+3\beta_1+5\beta_2\cr
          ...\cr
         \end{pmatrix}
\]
\paragraph{}
$x^T(y-x\beta)=0\Leftrightarrow (x^Tx)\beta=x^Ty$
\newline
Si $x^Tx$ est inversible $\Rightarrow \beta=(x^Tx)^{-1}x^Ty$
\subsection{théorème}
Soit A nxr, les propriétés suivantes sont équivalentes
\begin{enumerate}
 \item $\forall b\in R^n$ l'équation $Ax\sim b$(on approxime Ax par b) admet une solution unique au sens des moindres carrés
 \item les colonnes de A sont linéairement indépendantes
 \item $A^TA$ est inversible
\end{enumerate}
\subsection{Diagolalisation des matrices symétriques}
Soit A nxn, A est symétrique si $A^T=A$
\paragraph{exemple}
\[
A=\begin{pmatrix}
   2&1\cr
   1&3\cr
  \end{pmatrix}
  ,
  \begin{pmatrix}
   2&0\cr
   0&3\cr
  \end{pmatrix}
\]
\newline
sont deux matrices symétriques
\paragraph{théorème}
Deux vecteurs propres d'une matrice symétrique appartenant à des sous-espaces propres distincts sont orthogonaux
\paragraph{preuve}
$A^T=A$ avec deux valeurs propres $\lambda_1\neq\lambda_2, _1,v_2$
\newline
$(Av_1)v_2=(\lambda_1v_1)v_2$
\newline
$(Av_1)^Tv_2=v_1^TA^Tv_2=v_1^TAv_2=v_1^T(\lambda_2v_2)=\lambda_2v_1v_2$
\newline
comme $\lambda_1\neq \lambda_2\Rightarrow (\lambda_1-\lambda_2)v_1v_2=0\Rightarrow v_1\bot v_2$
\subsection{diagonalisation en une base orthonormée}
\paragraph{définition}
On dit qu'une matrice A nxn est diagonalisable en base orthonormée s'il existe une matrice P orthogonale tel que $D=P^{-1}AP$ est diagonale.
\subsubsection{propriétés}
Si A est diagonalisable en une base orthonormée, alors A est symétrique
\paragraph{preuve}
$A=PDP^{-1}$ ou P est est orthogonale: $PP^T=I\Leftrightarrow P^T=P^{-1}$
\newline
$A=PDP^T$
\newline
$A^T=(PDP^T)^T=(P^T)^Td^Tp^T=PDP^T$
\subsubsection{théorème}
A est diagonalisable en base orthonormée si et seulement si A est symétrique.
\newline
(que nous acceptons sans démonstration)
\paragraph{exemple}
\[
A=\begin{pmatrix}
   7&-2&0\cr
   -2&6&-2\cr
   0&-2&5\cr
  \end{pmatrix}
\]
\newline
$det(A-\lambda I)=det\begin{pmatrix}
                      7-\lambda&-2&0\cr
                      -2&6-\lambda&-2\cr
                      0&-2&5-\lambda\cr
                     \end{pmatrix}
$
\newline
$\lambda_1=3,\lambda_2=6,\lambda_3=9$
\newline
\[
v_1=\begin{pmatrix}
     1\cr
     2\cr
     2\cr
    \end{pmatrix}
,v_2=\begin{pmatrix}
      -2\cr
      -1\cr
      2\cr
     \end{pmatrix}
,v_3=\begin{pmatrix}
      2\cr
      -2\cr
      1\cr
     \end{pmatrix}
\]
\newline
$u_1=\frac{v_1}{||v_1||}=\begin{pmatrix}
                          \frac{1}{3}\cr
                          \frac{2}{3}\cr
                          \frac{2}{3}\cr
                         \end{pmatrix}
,u_2=\begin{pmatrix}
      -\frac{2}{3}\cr
      -\frac{1}{2}\cr
      \frac{2}{3}
     \end{pmatrix}
,u_3=\begin{pmatrix}
      \frac{2}{3}\cr
      -\frac{2}{3}\cr
      \frac{1}{3}\cr
     \end{pmatrix}
$
\newline
$P=[u_1u_2u_3]$ est orthogonale. A est diagonalisable en une base orthonormée($P^{-1}AP$)
\paragraph{remarque}
\begin{enumerate}
 \item Les racines de $P_A(\lambda)$ sont réelles
 \item La mutiplicité d'une valeur propre est la dimension du sous espace-propre
\end{enumerate}
\paragraph{théorème spectral}
toute matrice symétrique vérifie les conditions suivantes:
\begin{enumerate}
 \item A admet n valeurs propres réelles compte tenu des ordres de multiplicité
 \item Pour chaque valeur propre $\lambda$ la mulitiplicité de la valeur propre est la dimension du sous-espace propre
 \item A est diagonalisable en base orthonormée
\end{enumerate}
le spectre de A est l'ensemble des valeurs propres
\newline
Alors: A=$\lambda_1u_1u_1^T+\lambda_2u_2u_2^T+...+\lambda_nu_nu_n^T$
\paragraph{théorème}
Soit A une matrice mxn, alors les colonnes de A sont linéairement indépendantes sssi $A^TA$ est inversible.
\newline
preuve
\newline
Supposons que les colonnes de A sont linéairement indépendantes. Il nous faut démonstrer que Ker($A^TA$)=0.
\newline
Supposons que $x\in Ker(A^TA)\Rightarrow A^TAx=0$
\newline
$||Ax||^2=(Ax)^T(Ax)=x^T(A^TAx)=0\Rightarrow Ax=0$
\newline
$\Rightarrow x=0$ car les colonnes de A sont linéairement indépendantes donc $A^TAx=0$
\subsection{les formes quadratiques}
Nosu avons besoin de faire un lien entre les fonctions quadratiques et les matrices
\newline
\[
\alpha x_1^2+\beta x_2^2=||\begin{pmatrix}
               x_1\cr
               x_2\cr
              \end{pmatrix}
||^2=[x_1,x_2]\begin{pmatrix}
               \alpha&0\cr
               0&\beta\cr
              \end{pmatrix}
\begin{pmatrix}
               x_1\cr
               x_2\cr
              \end{pmatrix}
=x^Tx
\]
\subsubsection{définition de la forme quadratique}
Soit  A une matrice carrée nxn, alors la forme quadratique Q associée à A est $Q(x_1,...,x_n):=x^TAx$ ou $x=\begin{pmatrix}
                                                                                                             x_1\cr
                                                                                                             ...\cr
                                                                                                             x_n\cr
                                                                                                            \end{pmatrix}
$
\paragraph{exemple}
\[
A=\begin{pmatrix}
   1&1\cr
   0&1\cr
  \end{pmatrix}
\]
\newline
$Q(x_1,x_2)=[x_1 x_2]\begin{pmatrix}
                      1&1\cr
                      0&1\cr
                     \end{pmatrix}
\begin{pmatrix}
 x_1\cr
 x_2\cr
\end{pmatrix}
$
\newline
\[
=[x_1x_2]\begin{pmatrix}
          x_1+x_2\cr
          x_2\cr
         \end{pmatrix}
=x_1^2+x_1x_2+x_2^2
\]
\newline
Polynome homogène de degré 2 en deux variables
\newline
Si mnt nous prenions
\newline
\[
B=\begin{pmatrix}
   1&\frac{1}{2}\cr
   \frac{1}{2}&1\cr
  \end{pmatrix}
\]
\newline
$x^TBx=Q(x_1,x_2)$
\newline
Le but est de mettre la matrice sous forme symétrique.
\newline
--------------------------------
\newline
$A^TA$ nxn (n=3)
\newline
$\lambda_1=360,\lambda_2=90,\lambda_3=0$
\newline
$||Av_1||=6\sqrt{10}=\sqrt{360}$
\newline
$\sigma=\lambda^{\frac{1}{2}}$
\begin{description}
 \item $\sigma$, sigma:\\{soit A une matrice mxn et soit \{$v_1,...,v_n$\}une base orthonormée de $A^tA$ des vecteurs propres dont les valeurs propres $\lambda_1\geq...\geq\lambda_n\geq 0$. Alors, on définit la valeur singulière comme $\sigma_1,...,\sigma_n$ ou $\sigma_i= \lambda_i^{\frac{1}{2}}$}
\end{description}
\paragraph{théorème}
$\{v_1,..,v_n\}$ et $\lambda_1\geq ...\geq \lambda_n\geq 0$ comme dans la définition
\newline
Nous supposons qu'il existe $r\leq n$ valeurs $\sigma_i$ non-nulles
\newline
Alors, $\{Av_1,...,Av_r\}$ est une base orthogonale de Im(A) et rang(A)=r
\paragraph{preuve}
$(Av_i)(Av_j)=0$ (pour que les vecteurs soient orthogonaux)
\newline
$=(Av_i)T(Av_j)=0\Leftrightarrow v_i(A^TAv_j)=v_i(\lambda_jv_j)=\lambda_jv_iv_j=0$
\newline
On sait qu'une famille de vecteurs orthogonaux non-nuls est une famille libre
\newline
Disons qu'on prend $\{Av_1,...,Av_n\}$ $||Av_i||^2=v_i^TA^Tv_i=\lambda_iv_i^Tv_i=\lambda_i||v_i||^2=\lambda_i\Rightarrow Av_i\neq 0 si et seulement si \lambda_i\neq0\Leftrightarrow v_i\neq 0$
\newline
$\Rightarrow F\{Av_1,...,Av_r\}$ est une famille de vecteurs non-nuls orth $\Rightarrow$ libre$\Rightarrow\{Av_1,Av_r\}$ est une base de l'image
\subsection{décomposition en valeur singulière (SVD)}
Soit A mxn des valeurs singulières $\sigma_1\geq ...\geq \sigma_r\geq 0$, $\sigma_{r+1}=...=\sigma_n=0$
\newline
soit D la matrice diagonale avec les différents $\sigma$ dans sa diago
\newline
et soit $\sum$ une matrice mxn telle que 
%schéma1
\newline
Soit V=$[v_1...v_n]$ la matrice orthogonale nxn ($VV^T=I$)
\newline
Soit $\{u_1,...,u_r\}$ la base orthonormée obtenue par $\frac{Av_i}{||Av_i||},1\leq i\leq r$
\newline
On peut compléter $\{u_1...u_r\}$ pour obtenir une base orthonormée \{$u_1...u_r,u_{r+1}...u_n$\}
\newline
alors $U=[u_1...u_m]$
\newline
Alors $A=U\sum V^T$ (décomposition en valeurs singulières)
\paragraph{preuve}
voir le cours/livre
\section{examen}
va jusqu'aux formes quadatiques (7.2)
\newline
nous n'avons pas de SVD ou de statistique


\end{document}

